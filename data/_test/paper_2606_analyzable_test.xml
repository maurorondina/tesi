<?xml version="1.0" encoding="utf-8"?>
<paper id="2606" url="http://www.aaai.org/Papers/AAAI/2000/AAAI00-055.pdf">
 <title id="2606.0" name="Decision-Theoretic, High-level Agent Programming in the Situation Calculus"/>
 <subsection id="2606.0.1">
  Craig Boutilier Dept. of Computer Science University of Toronto Toronto, ON M5S 3H5 cebly@cs.toronto.edu Ray Reiter Dept. of Computer Science University of Toronto Toronto, ON M5S 3H5 reiter@cs.toronto.edu Mikhail Soutchanski Dept. of Computer Science University of Toronto Toronto, ON M5S 3H5 mes@cs.toronto.edu Sebastian Thrun School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 thrun@cs.cmu.edu
 </subsection>
 <paragraph id="2606.1" name="Abstract">
  <subsection id="2606.1.1">
   We propose a frameworkfor robot programming which allows the seamless integration of explicit agent programming with decision-theoretic planning. Specifically, the DTGolog model allows one to partially specify a control program in a highlevel, logical language, and provides an interpreter that, given a logical axiomatization of a domain, will determine the optimal completion of that program (viewed as a Markov decision process). We demonstrate the utility of this model with results obtained in an office delivery robotics domain.
  </subsection>
 </paragraph>
 <paragraph id="2606.2" name="1 Introduction">
  <subsection id="2606.2.1">
   The construction of autonomous agents, such as mobile robots or software agents, is paramount in artificial intelligence, with considerable research devoted to methods that will ease the burden of designing controllers for such agents.
  </subsection>
  <subsection id="2606.2.2">
   There are two main ways in which the conceptual complexity of devising controllers can be managed. The first is to provide languages with which a programmer can specify a control program with relative ease, using high-level actions as primitives, and expressing the necessary operations in a natural way. The second is to simply specify goals (or an objective function) and provide the agent with the ability to plan appropriate coursesof action that achieve those goals (or maximize the objective function). In this way the need for explicit programming is obviated.
  </subsection>
  <subsection id="2606.2.3">
   In this paper, we propose a framework that combines both perspectives, allowing one to partially specify a controller by writing a program in a suitably high-level language, yet allowing an agent some latitude in choosing its actions, thus requiring a modicum of planning or decision-making ability. Viewed differently, we allow for the seamless integration of programming and planning. Specifically, we suppose that the agent programmer has enough knowledge of a given domain to be able to specify some (but not necessarily all) of the structure and the details of a good (or possibly optimal) controller. Those aspects left unspecified will be filled in by the agent itself, but must satisfy any constraints imposed by the program (or partially-specified controller). When controllers can easily be designed by hand, planning has no role to play.
  </subsection>
  <subsection id="2606.2.4">
   On the other hand, certain problems are more easily tackled by specifying goals and a declarative domain model, and allowing the agent to plan its behavior.
  </subsection>
  <subsection id="2606.2.5">
   Copyright c 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.
  </subsection>
  <subsection id="2606.2.6">
   Our framework is based on the synthesis of Markov decisions processes (MDPs) [4, 13] with the Golog programming language [10]. Key to our proposal is the extension of the Golog language and interpreter, called DTGolog, to deal with uncertainty and general reward functions. The planning ability we provide is that of a decision-theoretic planner in which choices left to the agent are made by maximizing expected utility. Our framework can thus be motivated in two ways.
  </subsection>
  <subsection id="2606.2.7">
   First, it can be viewed as a decision-theoretic extension of the Golog language. Golog is a high-level agent programming language based on the situation calculus, with a clear semantics, and in which standard programming constructs (e.g., sequencing, nondeterministic choice) are used to write highlevel control programs.
  </subsection>
  <subsection id="2606.2.8">
   From a different standpoint, our contribution can be viewed as a language and methodology with which to provide “advice” to a decision-theoretic planner. MDPs are a conceptually and computationally useful model for decisiontheoretic planning, but their solution is often intractable. We provide the means to naturally constrain the search for (ideally, optimal) policies with a Golog program. The agent can only adopt policies that are consistent with the execution of the program. The decision-theoretic Golog interpreter then solves the underlying MDP by making choices regarding the execution of the program through expected utility maximization. This viewpoint is fruitful when one considers that an agent’s designer or “taskmaster” often has a good idea about thegeneral structureof a good (or optimal) policy, but maybe unableto commit to certain details. While we run the risk that the program may not allow for optimal behavior, this model has the clear advantage that the decision problem faced will generally be more tractable: it need only make those choices left open to it by the programmer. In contrast to existing models for constraining policies in MDPs, which use concepts such as local policies [11, 18] or finite-state machines [11], DTGolog provides a natural and well-understood formalism for programming behaviors.
  </subsection>
  <subsection id="2606.2.9">
   Our approach is specifically targeted towards developing complex robotics software. Within robotics, the two major paradigms—planning and programming—have largely been pursued independently. Both approaches have their advantages (flexibility and generality in the planning paradigm, performance of programmed controllers) and scaling limitations (e.g., the computational complexity of planning approaches, task-specific design and conceptual complexity for programmers in the programming paradigm). MDP-style planning has been at the core of a range of fielded robot apFrom: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved.
  </subsection>
  <subsection id="2606.2.10">
   plications, such as two recent tour-guide robots [5, 19]. Its ability to cope with uncertain worlds is an essential feature for real-world robotic applications. However, MDP planning scales poorly to complex tasks and environments. By programming easy-to-code routines and leaving only those choices to the MDP planner that are difficult to program (e.g., because the programmer cannot easily determine appropriate or optimal behavior), the complexity of planning can be reduced tremendously. Note that such difficult-to-program behaviors may actually be quite easy to implicitly specify using goals or objectives.
  </subsection>
  <subsection id="2606.2.11">
   To demonstrate the advantage of this new framework, we have developed a prototype mobile office robot that delivers mail, using a combination of pre-programmed behavior and decision-theoretic deliberation. An analysis of the relative trade-offs shows that the combination of programming and planning is essential for developing robust, scalable control software for robotic applications like the one described here.
  </subsection>
  <subsection id="2606.2.12">
   We give brief overviews of MDPs and Golog in Sections 2 and 3. We describethe DTGolog representation of MDPs and programs and the DTGolog interpreter in Section 4, and illustrate the functioningof the interpreterby describing its implementation in a office robot in Section 5.
  </subsection>
 </paragraph>
 <paragraph id="2606.3" name="2 Markov Decision Processes">
  <subsection id="2606.3.1">
   We begin with some basic background on MDPs (see [4, 13] for further details). We assume that we have a stochastic dynamical system to be controlled by some agent. A fullyobservable MDP    comprises the following components.  is a finite set of states of the system being controlled. The agent has a finite set of actions with which to influence the system state. Dynamics are given by  !#"$; here %')(*+,*'.-/ denotes the probability that action + , when executed at state ' (, induces a transition to '. 0,132 is a real-valued, bounded reward function. The process is fully observable: though the agent cannot predict the outcome of an action with certainty, it can observe that state precisely once it is reached.
  </subsection>
  <subsection id="2606.3.2">
   The decision problem faced by the agent in an MDP is that of forming an optimal policy (a mapping from states to actions) that maximizes expected total accumulated reward over some horizon of interest. An agent finding itself in state ')4 at time 5 must choose an action +64. The expected value of a course of action 7 depends on the specific objectives. A finite-horizon decision problem with horizon 8 measures the value of 7 as 9:%;=4@? A%B'C4D/#E7@/ (where expectation is taken w.r.t. ).F For an MDP with horizon 8 , a (nonstationary) policy 7GIHJLKM"N#OO#OD.8P RQ associates with each state ' and stage-to-go 5TSU8 an action 7%'6V5*/ to be executed at ' with 5 stages remaining. An optimal policy is one with maximum expected value at each state-stage pair.
  </subsection>
  <subsection id="2606.3.3">
   The planning problem faced by an agent is that of forming an optimalpolicy (a mappingfrom states to actions)that maximizes expected total accumulated reward over some horizon. Dynamic programming methods are often used to solve WWe focus on finite-horizon problems to keep the presentation short, though everything we describe can be applied with little modification to discounted, infinite-horizon MDPs.
  </subsection>
  <subsection id="2606.3.4">
   MDPs [13], though one difficulty facing (the classical versions of) such algorithms is their reliance on an explicit statespace formulation; as such, their complexity is exponential in the number of state variables. However, “logical” representations such as STRIPS and dynamic Bayesian networks have recently been used to make the specification and solution of MDPs much easier [4]. The DTGolog representation goes further in this direction by specifying state transitions in first order logic. Restricting attention to reachable states using decision tree search can, in some circumstances, alleviate the computational difficulties of dynamic programming.
  </subsection>
  <subsection id="2606.3.5">
   Search-based approaches to solving MDPs can use heuristics, learning, sampling, and pruning to improve their efficiency [3, 6, 7, 8, 9]. Declarative search control knowledge, used successfully in classical planning[2], might also be used to prune the search space. In an MDP, this could be viewed as restricting the set of policies considered. This type of approachhas been exploredin themore general context of value iteration for MDPs in, e.g., [11, 18]: local policies or finitestate machines are used to model partial policies, and techniques are devised to find the optimal policy consistent with the constraints so imposed. In Section 4 we develop the DTGolog interpreter to capture similar intuitions, but adopt the Golog programming language as a means of specifying these constraints using natural programming constructs.
  </subsection>
 </paragraph>
 <paragraph id="2606.4" name="3 The Situation Calculus and Golog">
  <subsection id="2606.4.1">
   The situation calculus is a first-order language for axiomatizing dynamic worlds. In recent years, it has been considerably extended beyond the “classical” language to include concurrency, continuous time, etc., but in all cases, its basic ingredients consist of actions, situations and fluents.
  </subsection>
  <subsection id="2606.4.2">
   Actions are first-order terms consisting of an action function symbol and its arguments. In the approach to representing time in the situation calculus of [14], one of the arguments to such an action function symbol—typically, its last argument—is the time of the action’s occurrence. For example, startGo%YXDDXYZ[\!]^"#/ might denote theaction of a robot starting to move from location X to X_Z at time 3.1. Following Reiter [14], all actions are instantaneous (i.e, with zero duration).` A situation is a first-order term denoting a sequence of actions. These sequences are represented using a binary function symbol do: do%Bab*'#/ denotes the sequence resulting from adding the action a to the sequence '. The special constant H ? denotes the initial situation, namely the empty action sequence. Therefore, the situation term do%endGo%_XD.X_ZVc!]\N/)do%startGrasp%dMeN/C do%startGo%YXDDXYZ[eN/)*H ? /D/D/ denotes the following sequence of actions: startGof_ghVg_i^hDj#k, startGraspf_lNhDj#k, endGof_gBhVgih*monpk. Axioms for situations with time are given in [15].
  </subsection>
  <subsection id="2606.4.3">
   Relations whose truth values vary from state to state are called relational fluents, and are denoted by predicate or functionsymbolswhose lastargument is a situation term. For qDurations can be captured using processes, as shown below. A full exposition of time is not possible here.
  </subsection>
  <subsection id="2606.4.4">
   example, rXBd's)8 dM%tu*vw*'#/ might be a relational fluent, meaning that x when the robot performs the action sequence denoted by the situation term ', t will be close to v .
  </subsection>
  <subsection id="2606.4.5">
   A domain theory is axiomatized in the situation calculus with four classes of axioms: Action precondition axioms: There is one for each action function Qy%#zt,/, with syntactic form Poss%_QA%#zt,/C'/|{ }~ %#ztu*'#/C] Here, }~ %#ztu*'#/ is a formula with free variables among zt'6]These are the preconditions of action Q .
  </subsection>
  <subsection id="2606.4.6">
   Successor state axioms: There is one for each relational fluent :%#zt'#/, with syntactic form :%#ztu*MdM%B+'#/D/:{ b %#zt+,*'#/C where b %#zt+,*'#/ is a formula with free variables among +,*'6Mzt ]These characterize the truth values of the fluent  in the next situation MdM%+,'/ in terms of the current situation ', and they embody a solution to the frame problem for deterministic actions ([16]).
  </subsection>
  <subsection id="2606.4.7">
   Unique names axioms for actions: These state that the actions of the domain are pairwise unequal.
  </subsection>
  <subsection id="2606.4.8">
   Initial database: This is a set of sentences whose only situation term is H ? ; it specifies the initial problem state.
  </subsection>
  <subsection id="2606.4.9">
   Examples of these axioms will be seen in Section 4.1.
  </subsection>
  <subsection id="2606.4.10">
   Golog [10] is a situation calculus-based programming language for defining complex actions in terms of a set of primitive actions axiomatized in the situation calculus as described above. It has the standard—and some not-so-standard— control structures found in most Algol-like languages.
  </subsection>
  <subsection id="2606.4.11">
   1. Sequence: a@] Do action a , followed by action  .
  </subsection>
  <subsection id="2606.4.12">
   2. Test actions: , Test the truth value of expression  in the current situation.
  </subsection>
  <subsection id="2606.4.13">
   3. Nondeterministic action choice: aE#] Do a or  .
  </subsection>
  <subsection id="2606.4.14">
   4. Nondeterministic choice of arguments: %7t/.ab%t,/. Nondeterministically pick a value for t , and for that value of t , do action ab%t,/.
  </subsection>
  <subsection id="2606.4.15">
   5. Conditionals (if-then-else) and while loops.
  </subsection>
  <subsection id="2606.4.16">
   6. Procedures, including recursion.
  </subsection>
  <subsection id="2606.4.17">
   The semantics of Golog programs is defined by macroexpansion, using a ternary relation Do. Do%_'N' Z/ is an abbreviation for a situation calculus formula whose intuitive meaning is that 'CZ is one of the situations reached by evaluating the program  beginning in situation '. Given a program  , one proves, using the situation calculus axiomatization of the background domain, the formula %!'#/Do%_H ? '/ to compute a plan. Any binding for ' obtained by a constructive proof of this sentence is a legal execution trace, involving only primitive actions, of  . A Golog interpreter for the situation calculus with time, implemented in Prolog, is described in [15].
  </subsection>
  <subsection id="2606.4.18">
   Thus the interpreter will makes choices (if possible) that lead to successful computation of an execution trace of the program. With nondeterministic choice and the specification of postconditions corresponding to goals, Golog can be viewed as integrating planning and programming in deterministic domains. We will see examples of Golog programs in Section 5.
  </subsection>
 </paragraph>
 <paragraph id="2606.5" name="4 DTGolog: Decision-Theoretic Golog">
  <subsection id="2606.5.1">
   As a planning model, MDPs are quite flexible and robust, dealing with uncertainty, multiple objectives, and so on, but suffer from several key limitations. While recent work in DTP has focused on the development of compact, natural representations for MDPs [4], little work has gone into the development of first-order languages for specifying MDPs (see [1, 12] for two exceptions). More importantly, the computational complexity of policy construction is prohibitive.
  </subsection>
  <subsection id="2606.5.2">
   As mentioned, one way to circumvent planning complexity is to allow explicit agent programming; yet little work has been directed toward integrating the ability to write programs or otherwise constrain the space of policies that are searched during planning. What work has been done (e.g., [11, 18]) fails to provide a language for imposing such constraints, and certainlyoffers no tools for programming agent behavior. We believe that natural, declarative programming languages and methodologies for (partially) specifying agent behavior are necessary for this approach to find successful application in real domains.
  </subsection>
  <subsection id="2606.5.3" label="Problem">
   Golog, on the other hand, provides a very natural means for agent programming. With nondeterministic choice a programmer can even leave a certain amount of “planning” up to the interpreter (or agent being controlled). However, for applications such as robotics programming, the usefulness of Golog is severely limited by its inability to model stochastic domains, or reason decision-theoretically about appropriate choices. Despite these limitations, (deterministic) Golog has been successfully used to provide the high-level control of a museum tour-guide robot, controlling user interaction and scheduling more than 2,400 exhibits [5].
  </subsection>
  <subsection id="2606.5.4" label="Problem">
   We have developed DTGolog, a decision-theoretic extension of Golog that allows one to specify MDPs in a first-order language, and provide “advice” in the form of high-level programs that constrain the search for policies. A program can be viewed as a partially-specified policy: its semantics can be viewed, informally, as the execution of the program (or the completion of the policy) that has highest expected value.
  </subsection>
  <subsection id="2606.5.5" label="Problem">
   DTGolog offers a synthesis of both planning and programming, and is in fact general enough to accommodate both extremes. One can write purely nondeterministic programs that allow an agent to solve an MDP optimally, or purely deterministic programs that leave no decisionsin the agent’s hands whatsoever. We will see, in fact, that a point between these ends of the spectrum is often the most useful way to write robot programs. DTGolog allows the appropriate point for any specific problem to be chosen with relative ease. Space precludes the presentation of many technical details, but we try to provide the basic flavor of DTGolog.
  </subsection>
  <subsection id="2606.5.6">
   4.1 DTGolog: Problem Representation Thespecification of an MDP requires the provision of a background action theory—as in Section 3—and a background optimization theory—consisting of the specification of a reward function and some optimality criterion (here we require only a horizon 8 ). The unique names axioms and initial database have the same form as in standard Golog.
  </subsection>
  <subsection id="2606.5.7">
   A background action theory in the decision-theoretic setting  distinguishes between deterministic agent actions and stochastic agent actions. Both types are used to form programs and policies. However, the situation resulting from execution of a stochastic action is not determined by the action itself: instead each stochastic agent action is associated with a finite set of deterministic actions, from which “nature” chooses stochastically. Successor state axioms are provided for nature’s actions directly (which are deterministic), not for stochastic agent actions (i.e., successor state axioms never mention stochastic agent actions). When a stochastic action is executed, nature chooses one of the associated actions with a specified probability, and the successor state is given by nature’s action so chosen. The predicate stochastic%+,'N@/ relates a stochastic agent action + to one of nature’s action  in a situation ', and prob%Bu*'#/ denotes the probability with which  is chosen in '. Deterministic agent’s actions are axiomatized using exactly the same precondition and successor state axioms. This methodology allows us to extend the axiomatization of a domain theory described in the previous section in a minimal way.
  </subsection>
  <subsection id="2606.5.8">
   As an example, imagine a robot moving between different locations: the process of going is initiated by a deterministic action startGo%YXF .X` V5*/; but the terminating action endGo%_XF .X` 5*/ is stochastic (e.g., the robot may end up in some location other than X`, say, the hallway). We give nature two choices, endGoS%_XF .X` 5*/ (successful arrival) and endGoF%_XF HallV5*/ (end with failure), and include axioms such as stochastic%endGo%YXF .X` 5*/C'NendGoS%_XF DX` V5*/./ and prob%endGoS%YXF .X` 5*/C!]!'/ (i.e., successful movementoccurs with probability0.9 in any situation). Let going%_XF DX` '/ be the relational fluent meaning that in the situation ' the robot is in the process of moving between locations XF and X` ; and let robotLoc%YXD*'#/ be a relational fluent denoting the robot’s location. The following precondition and successor state axioms characterize these fluents, and the actions startGo, endGoS, endGoF: PossfstartGof_gWhVgq hDBk.hDCkfgBhVgikgoingf_gBhDgihDCk  robotLocf_gWhDCk PossfendGoSf_gW hVgq hDk.hDCku goingf_gW hVgq hDCk.h PossfendGoFf_gW h[gq h*k.hVCkogingoingf_gWhDgihDCk  gi@ gq goingf_gBh[g_ihDlNf_MhVCkVkf6kB  startGof_gBh[g_ih*k@ goingf_gBh[g_ih)k  fNBkB  endGoSf_gBh[g_ihBkV goingf_gBh[gih)k  fNhVgiikB  endGoFf_gBhVgiihDk.h The background action theory also includes a new class of axioms, sense conditions axioms, which assert atomic formulae using predicate senseCond%@/: this holds if  is a logical condition that an agent uses to determine if the specific nature’s action  occurred when some stochastic action was executed. We require such axioms in order to “implement” full observability. While in the standard MDP model one simply assumesthat the successor state is known, in practice, one must force agents to disambiguate the state using sensor information. The sensing actions needed can be determined from sense condition axioms. The following distinguish successful from unsuccessful movement: senseCond%endGoS%YXF .X` 5*/CrobotLoc%YX` /./ senseCond%endGoF%YXF .X` V5*/)robotLoc%Hall/./ A DTGolog optimization theory contains axioms specifying the reward function. In their simplest form, reward axioms use the function reward%B'/ and assert costs and rewards as a function of the action taken, properties of the current situation, or both (note that the action taken can be recovered from the situation term). For instance, we might assert reward%do%giveCoffeeSuccessful%JillV5*/)*'#/D/J¡!]\ Because primitive actions have an explicit temporal argument, we can also describe time-dependent reward functionseasily (associated with behaviorsthat extend overtime).
  </subsection>
  <subsection id="2606.5.9">
   These can be dealt with in the interpreter because of our use of situation terms rather than states, from which time can be derived without having it explicitly encoded in the state. This often proves useful in practice. In a given temporal Golog program, the temporal occurrence of certain actions can be uniquely determined either by temporal constraints or by the programmer. Other actions may occur at any time in a certain interval determined by temporal inequalities; for any such action QA%#ztuV5*/, we can instantiate the time argument by maximizing the reward for reaching the situation do%_Qy%#zt5*/C'#/. For example, suppose the robot receives a reward ¢ G£¤+Mtu% F??)¥4 ¦(_§4¨©6ªD«¬®­^¯V°­®±² / for doing the action endGoS%_XF .X` 5*/ in '. With this reward function, the robot is encouraged to arrive at the destination as soon as possible and is also encouraged to go to nearby locations (because the reward is inversely proportional to distance).
  </subsection>
  <subsection id="2606.5.10">
   Our representation for stochastic actions is related somewhat to the representations proposed in [1, 7, 12].
  </subsection>
  <subsection id="2606.5.11">
   4.2 DTGolog: Semantics In what follows, we assume that we have been provided with a background action theory and optimization theory. We interpret DTGolog programs relative to this theory. DTGolog programs are written using the same program operators as Golog programs. The semantics is specified in a similar fashion, with the predicate BestDo (described below) playing the role of Do. However, the structure of BestDo (and its Prolog implementation) is rather different than that of Do. One difference reflects the fact that primitive actions can be stochastic. Execution traces for a sequence of primitive actions need not be simple “linear” situation terms, but rather branching “trees.” Another reflects the fact that DTGolog distinguishes otherwise legal traces according to expected utility. Given a choice between two actions (or subprograms) at some point in a program, the interpreter chooses the action with highest expected value, mirroring the structure of an MDP search tree. The interpreter returns a policy—an expanded Golog program—in which every nondeterministic choice point is grounded with the selection of an optimal choice. Intuitively, the semantics of a DTGolog program will be given by the optimal execution of that program.
  </subsection>
  <subsection id="2606.5.12">
   The semantics of a DTGolog program is defined by a predicate BestDo%®,¢d)³´'Nµd¢¶B·!Bd#X.*¸6+NX.,¢d¹C/, where ,¢d)³ is a Golog program, ' is a starting situation, d#X is the optimal conditional policy determined by program ¢d)³ beginning in ºWe require an optimality criterion to be specified as well. We assume a finite-horizon » in this work.
  </subsection>
  <subsection id="2606.5.13">
   situation ', ¸6+6X is the expected value of that policy,,¢d¹ is the probability ¼ that ,d#X will execute successfully, and µd¢¶B· is a prespecified horizon. Generally, an intepreter implementing this definition will be called with a given program ¢d)³ , situation H ?, and horizon µd¢¶B· , and the arguments d#X, ¸6+6X and ,¢d¹ will be instantiated by the interpreter. The policy ,d#X returned by the interpreter is a Golog program consisting of the sequential composition (under ) of agent actions, senseEffect%_Q/ sensing actions (which serve to identify nature’s choices whenever Q is a stochastic agent action), and conditionals (if  then ,d#XF else ,d#X`).
  </subsection>
  <subsection id="2606.5.14">
   Below we assume an MDP with finite horizon ½ : if a program fails to terminate before the horizon is reached, the interpreter produces the best (partial) ½ -step execution of the program. The interpreter can easily be modified to deal with programs that are guaranteed to terminate in a finite amount of time (so a bound ½ need not be imposed) or infinitehorizon, discounted problems (returning ¾ -optimal policies).
  </subsection>
  <subsection id="2606.5.15">
   BestDo is defined inductively on the structure of its first argument, which is a Golog program: 1. Zero horizon.
  </subsection>
  <subsection id="2606.5.16">
   BestDo%¿'Nµ7¸!B¢/ ¦«VÀ µ¤GTÁ¤7TÃÂ¶YX ÁÄ¸u ¢s#Å +M¢,%'#/ÆÁ¢@Ç"N] Give up on the program  if the horizon reaches 0.
  </subsection>
  <subsection id="2606.5.17">
   2. The null program BestDo%BÂ¶_X.*'6*µu7*¸w,¢/ ¦«À 7TÃÂ¶YXÈÁG¸Ã¢s#Å +M¢,%'#/ÆÁL,¢@Ç"N] 3. First program action is deterministic.
  </subsection>
  <subsection id="2606.5.18">
   BestDo%B+B'6*µu*7¸!B¢/¦«VÀ  É Poss%+,'/@ÁÃ7Ã Stop ÁA¢@ÃÊÁÃ¸u reward%B'/Ë Poss%B+,*'#/DÁ @%7,ZV¸ZVB¢#ZÌ/BestDo%®MdM%+,'/)*µ –"6*7,Z_¸ZB,¢#ZÌ/DÁ 7Ã1+,7,ZNÁ ¸Í1¢s#Å +M¢,%'#/@Î¸ZÏÁA,¢ ¢#ZV] A program that begins with a deterministic agent action + (if + is possible in situation ') has its optimal execution defined as the optimal execution of the remainder of the program  in situation do%+,'/. Its value is given by the expected value of this continuation plus the reward in ' (action cost for + can be included without difficulty), while its success probability is given by the success probability of its continuation. The optimal policy is + followed by the optimal policy for the remainder. If + is not possible at ', the policy is simply the Stop action, the success probability is zero, and the value is simply the reward associated with situation '. H@5*d is a zero-cost action that takes the agent to a zero-cost absorbing state.Ð 4. First program action is stochastic.
  </subsection>
  <subsection id="2606.5.19">
   When + is a stochastic agent action for which nature selects one of the actions in the set K F #]]#]*,ÑMP , BestDo%B+B'6*µu*7¸!B¢/¦«VÀ  @%7,Z_/C]BestDoAux%VK F ^]]#] ÑPÏB'Nµu*7,Z^*¸w,¢/Á 7ÃG+senseEffect%B+M/C7,Z] ÒThis can be viewed as having an agent simply give up its attempt to execute the policy and await further instruction.
  </subsection>
  <subsection id="2606.5.20">
   The resulting policy is +,senseEffect%+M/C7 Z where 7 Z is the policy delivered by BestDoAux. Intuitively, this policy says that the agent should first perform action + , at which point nature selects one of  F ]#]#]* Ñ to perform (with probabilities ¢d¹%B ('#/), then the agent should sense the outcome of action + (which tells it which of nature’s actions  ( actually occurred), then it should execute the policy delivered by BestDoAux.Ó BestDoAux%VKPMu*'6µ7*¸w,¢/¦«VÀ  7TÃH@5*dAÁÃ¸ÃTÁA¢@Ã!] Suppose ÔÖÕ×". Suppose further that  F is the sense condition for nature’s action  F, meaning that observing that  F is true is necessary and sufficient for the agent to conclude that nature actually performed action  F, among the choices KØ F #]#]],ÑMP available to her by virtue of the agent having done stochastic action + . Then BestDoAux%KØ F #]#]],ÑMPÏB'Nµ7¸!B¢/¦«À  É Poss%B F '/)Á BestDoAux%KØ ` ^]]#] Ñ PÏB'Nµ7¸!B¢/ Ë Poss% F '/Á @%7,ZV¸ZVB¢#ZÌ/)]BestDoAux%KØ ` Ì]#]#]*,ÑMPMu*'6*µu7,ZÌ¸ZVB¢#ZÌ/Á @%7 F ¸ F B¢ F/C]BestDo%®MdM% F '#/Cµ –"N7 F ¸ F ,¢ F/IÁ 7Ã if  F then 7 F else 7,ZÁ ¸Ã¸Z!Î¸ F OD,¢d¹% F '#/Á ¢@A¢#ZNÎÇ,¢ F OD,¢d¹% F *'#/C] BestDoAux determinesa policy in the form of a conditional plan: if ,(¯ then ,d#XF else if ,(± then d#X` O#O#O else if ,(BÙ then ,d#X^Ú else H@5*du] Here, ,(¯ #]#]],(Ù are all of nature’s actions among KØ F #]]#]* Ñ P that are possible in ', and d#Xis the policy returned by the program  , in situation ÏdM%B (^Û *'#/.
  </subsection>
  <subsection id="2606.5.21">
   5. First program action is a test.
  </subsection>
  <subsection id="2606.5.22">
   BestDo%@MB'Nµu*7¸!B¢/¦«À  'Ü$ÏÁ BestDo%¿'Nµ7¸!B¢/@Ë É 'Ü$ÏÁÃ7T Stop ÁÆ¢@ÃTÁÃ¸Ã¢s#Å +M¢,%'#/ 6. First program action is the nondeterministic choice of two programs.
  </subsection>
  <subsection id="2606.5.23">
   BestDo%D%¿ F E ` /)u*'6*µu7*¸w,¢/¦«VÀ   %7 F ¸ F B¢ F/C]BestDo%® F u*'6µ7 F ¸ F B¢ F/Á  %7 ` ¸ ` B¢ ` /C]BestDo%® ` u*'6µ7 ` ¸ ` B¢ `/Á %.%¸ F ,¢ F/bS1%¸ ` B¢ ` /@ÁÃ7T 7 ` ÁÃ¸Ã¸ ` ÁA¢@A¢ ` Ë %¸ F ,¢ F/ÝÞ%¸ ` B¢ ` /@ÁÃ7T 7 F ÁÃ¸Ã¸ F ÁA¢@A¢ F/)] Given the choice between two subprograms  F and  `, the optimal policy is determined by that subprogram with optimal execution. Note that there is some subtlety in the interpretation of a DTGolog program: on the one hand, we wish the interpreter to choose a course of action with maximal expected value; on the other, it should follow the advice provided by the program. Because certain choices may lead to abnormal termination—the H@5*d action corßIt is these sensing actions that “implement” the assumption that the MDP is fully observable.
  </subsection>
  <subsection id="2606.5.24">
   responding to an incomplete execution of the program— with à varying probabilities, the success probability associated with a policy can be loosely viewed as the degree to which the interpreter adhered to the program. Thus we have a multi-objective optimization problem, requiring some tradeoff between success probability and expected value of a policy. The predicate S compares pairs of the form %®¸N/, where  is a success probability and ¸ is an expected value.á 7. Conditionals.
  </subsection>
  <subsection id="2606.5.25">
   BestDo%.%if  then  F else  ` /CB'Nµu*7¸!B¢/¦«À  BestDo%.%@MB F E É @MB `/CB'6*µu*7¸!B¢/ This simply says that a conditional if  then  F else  ` is an abbreviation for @MB F E É @MB `.
  </subsection>
  <subsection id="2606.5.26">
   8. Nondeterministic finite choice of action arguments.
  </subsection>
  <subsection id="2606.5.27">
   BestDo%.%7%tâNã!/Y/CB!ZB*'6µBd#XD¸!B¢/ ¦«À BestDo%¿IEäª¯ EO#OOÆEåIEäªæ /)wZV*'6*µuBd#XD¸!B¢/ The programming construct 7%BtÃNã!/Y requires the nondeterministic choice of an element t from the finite set ãÃ KØr F ]#]#]*r© P , and for that t , do the program  . It therefore is an abbreviation for the program Eäª¯ EÍOO#OÆEIEäªæ , where IEäª means substitute r for all free occurrences of t in  .
  </subsection>
  <subsection id="2606.5.28">
   9. Associate sequential composition to the right.
  </subsection>
  <subsection id="2606.5.29">
   BestDo%.%® F B ` /) *'6µ7*¸w,¢/ ¦«À BestDo%® F #%® ` B /C'Nµu*7¸!B¢/)] This is needed to massage the program to a form in which its first action is one of the forms suitable for application of rules 2-8.
  </subsection>
  <subsection id="2606.5.30">
   There is also a suitable expansion rule when the first program action is a procedure call. This is almost identical to the rule for Golog procedures [10], and requires second-order logic to characterize the standard fixed point definition of recursive procedures. Because it is a bit on the complicated side, and because it is not central to the specification of policies for DTGolog, we omit this expansion rule here. While loops can be defined using procedures.
  </subsection>
  <subsection id="2606.5.31">
   4.3 Computing Optimal Policies BestDo%¿¢d)³@*'6*µ,d¢¶B·!Bd#XD¸N+6XDB¢d¹C/ is, analogously to the case for Golog, an abbreviation for a situation calculus formula whose intuitive meaning is that ,d#X is an optimal policy resulting from evaluating the program ,¢d)³ beginning in situation ', that ¸6+NX is its value, and ¢d¹ the probability of a çHow one defines this predicate depends on how one interprets the advice embodied in a program. In our implementation, we use a mild lexicographic preference where fè WhDé Wkêfè q hDé q k whenever è W ìë and è qîí ë (so an agent cannot choose an execution that guarantees failure). If both è W and è q are zero, or both are greater than zero, than the é -terms are used for comparison. It is important to note that certain multiattribute preferences could violate the dynamic programming principle, in which case our search procedure would have to be revised (as would any form of dynamic programming). This is not the case with our lexicographic preference.
  </subsection>
  <subsection id="2606.5.32">
   successful execution of this policy. Therefore, given a program  , and horizon H, one proves, using the situation calculus axiomatization of the background domain described in Section 4.1, the formula @%¿d#XD¸N+6XDB¢d¹)/BestDo%_!Â¶YXD*H ? *½â,d#XD*¸6+6XD,¢d¹C/C] Any binding for d#X, ¸N+6X and ,¢d¹ obtained by a constructive proof of this sentence determines the result of the program computation.
  </subsection>
  <subsection id="2606.5.33">
   4.4 Implementing a DTGolog Interpreter Just as an interpreter for Golog is almost trivial to implement in Prolog, when given its situation calculus specification, so also is an interpreter for DTGolog. One simply translates each of the above rules into an almost identical Prolog clause.
  </subsection>
  <subsection id="2606.5.34">
   For example, here is the implementation for rules 3 and 6: % First action is deterministic.
  </subsection>
  <subsection id="2606.5.35">
   bestDo(A : E,S,H,Pol,V,Prob) :agentAction(A), deterministic(A), (not poss(A,S), Pol=stop, Prob is 0, reward(V,S); poss(A,S), bestDo(E,do(A,S),H-1,RestPol,Vfuture,Prob), reward(R,S), V is R + Vfuture, Pol = (A : RestPol)).
  </subsection>
  <subsection id="2606.5.36">
   % Nondeterministic choice between E1 and E2 bestDo((E1 # E2) : E,S,Pol,V,P,k) :bestDo(E1 : E,S,Pol1,V1,P1,k), bestDo(E2 : E,S,Pol2,V2,P2,k), ( lesseq(V1,P1,V2,P2), Pol=Pol2, P=P2, V=V2; greater(V1,P1,V2,P2), Pol=Pol1, P=P1, V=V1).
  </subsection>
  <subsection id="2606.5.37">
   The entire DTGolog interpreter is in this style, and is extremely compact and transparent.
  </subsection>
 </paragraph>
 <paragraph id="2606.6" name="5 Robot Programming">
  <subsection id="2606.6.1">
   A key advantage of DTGolog as a framework for robot programming and planning is its ability to allow behavior to be specified at any convenient point along the programming/planning spectrum. By allowing the specification of stochastic domain models in a declarative language, DTGolog not only allows the programmer to specify programs naturally (using robot actions as the base level primitives), but also permits the programmer to leave gaps in the program that will be filled in optimally by the robot itself. This functionality can greatly facilitate the development of complex robotic software. Planning ability allows for the scheduling of complex behaviors that are difficult to preprogram. It also obviatesthe need to reprogram a robot to adapt its behaviorto reflect environmental changes or changes in objective functions. Programming, in contrast, is crucial in alleviating the computational burden of uninformed planning.
  </subsection>
  <subsection id="2606.6.2">
   To illustrate these points, we have developed a mobile delivery robot, tasked to carry mail and coffee in our office building. The physical robot is an RWI B21 robot, equipped with a laser range finder. The robot navigates using BeeSoft [5, 19], a software package that includes methods for map acquisition, localization, collision avoidance, and online path planning. Figure 1d shows a map, along with a delivery path (from the main office to a recipient’s office).
  </subsection>
  <subsection id="2606.6.3">
   Initially, the robot moves to the main office, where someone loads mail on the robot, as shown in Figure 1a. DTGolog then chooses a recipient by utility optimization. Figure 1b shows the robot traveling autonomously through a hallway.
  </subsection>
  <subsection id="2606.6.4">
   If the person is in his office, he acknowledges the receipt of  (a) (b) (c) (d) Figure 1: Mail delivery: (a) A person loads mail and coffee onto the robot. (b) DTGolog sends the robot to an office. (c) The recipient accepts the mail and coffee, acknowledging the successful delivery by pressing a button. (d) The map learned by the robot, along with the robot’s path (from the main office to recipient).
  </subsection>
  <subsection id="2606.6.5">
   the items by pressing a button on the robot as shown in Figure 1c; otherwise, after waiting for a certain period of time, the robot marks the delivery attempt as unsuccessfuland continues with the next delivery. The task of DTGolog, thus, is to schedule the individual deliveries in the face of stochastic action effects arising from the fact that people may or may not be in their office at the time of delivery. It must also contend with different priorities for different people and balance these against the domain uncertainty.
  </subsection>
  <subsection id="2606.6.6">
   The underlying MDP for this relatively simple domain grows rapidly as the number of people requiring deliveries increases. The state space is characterized by fluents such as hasMail%person*'#/, mailPresent%person*'#/, robotLoc%_Xdr'#/, and so on. In a domain with ï people, ð locations, and Â as the maximum number of pieces of mail (and ignoring the temporal aspect of the problem), our MDP has a state space of size eñÞOM%¡6Â0ÎJ¡N/VòOð when formulated in the most appropriate way. Even restricting the MDP to one piece (or bundle) of mail per person, the state space complexity, eoóNòOð , grows exponentially in ï . Actions include picking up mail, moving from location to location, giving mail and so on. Uncertainty is associated with the endGo action as described above, as well as with the outcome of giving mail (see below).
  </subsection>
  <subsection id="2606.6.7">
   The robot’s objective function is given by a reward function that associates an independent, additive reward with each person’s successful delivery. Each person has a different deadline, and the reward decreases linearly with time until the deadline (when it becomes zero). The relative priority associated with different recipients is given by this function; e.g., we might use reward%+Mv!5*'#/1\6ôA5*õ!"# , where the initial reward (30) and rate of decrease (1/10) indicates relative priority. Given a situation term corresponding to any branch of the tree, it is straightforward to maximize value with respect to choice of temporal arguments assigned to actions in the sequence. We do not delve into details here.
  </subsection>
  <subsection id="2606.6.8">
   Our robot is provided with the following simple DTGolog program: while fè,n@NBYö÷Tè!YöCMfèøk  6ù mailPresent(p,n)k ú fè,hMèÏö)lDèMg^öh f@NB_ö#÷Tè!_ö)Mfèøk 6ù mailPresent(p,n)kVûåü@öCg_ý_é#öþÿl6fèøk@k endWhile Intuitively, this program chooses people from the finite range people for mail delivery and delivers mail in the order that maximizes expected utility (coffee delivery can be incorporated readily). deliverTo is itself a complex procedure involvingpicking up items for a person, moving to the person’s office, giving the items, and returning to the mailroom. But this sequence is a very obvious one to handcode in our domain, whereas the optimal ordering of delivery is not (and can change, as we’ll see). We have included a guard condition É +o5V5*s#£ÆM5*s#,%®,/IÁÃ! mailPresent(p,n) in the program to prevent the robot from repeatedly trying to deliver mail to a person who is out of her office. This program constrains the robot to just one attempted mail delivery per person, and is a nice example of how the programmer can easily impose domain specific restrictions on the policies returned by a DTGolog program.
  </subsection>
  <subsection id="2606.6.9">
   Several things emerged from the development of this code.
  </subsection>
  <subsection id="2606.6.10">
   First, the same program determines different policies— and very different qualitative behavior—when the model is changed or the reward function is changed. As a simple example, when the probability that Ray (high priority) is in his office is !] , his delivery is scheduled before Craig’s(low priority); but when that probability is lowered to !]¡ , Craig’sdelivery is scheduled beforehand. Such changes in the domain would require a change in the control program if not for the planning ability provided by DTGolog. The computational requirementsof this decisionmaking capabilityare much less than those should we allow completely arbitrary policies to be searched in the decision tree.
  </subsection>
  <subsection id="2606.6.11">
   Full MDP planning can be implemented within DTGolog by running it with the program that allows any (feasible) action to be chosen at any time. This causes a full decision tree to be constructed. Given the domain complexity, this unconstrained search tree could only be completely evaluated for problems with a maximum horizon of seven (in about 1 minute)—this depth is barely enough to complete the construction of a policy to serve one person. With the program above, the interpreter finds optimal completions for a 3-person domain in about 1 second (producing a policy with success probability 0.94), a 4-person domain in about 9 seconds (success probability 0.93) and a 5-person domain in about 6 minutes (success probability 0.88). This latter corresponds to a horizon of about 30; clearly the decision tree search would be infeasible without the program constraints (withsize wellover "#C?). We notethat the MDPformulation of this problem, with 5 people and 7 locations, would require  more than 2.7 billion states. So dynamic programming could not be used to solve this MDP without program constraints (or exploiting some other form of structure).
  </subsection>
  <subsection id="2606.6.12">
   We note that our example programs restrict the policy that the robot can implement, leaving only one choice (the choice of person to whom to deliver mail) available to the robot, with the rest of the robot’s behavior fixed by the program.
  </subsection>
  <subsection id="2606.6.13">
   While these programs are quite natural, structuring a program this way may preclude optimal behavior. For instance, by restricting the robot to serving one person at a time, the simultaneous delivery of mail to two people in nearby offices won’t be considered. In circumstances where interleaving is impossible (e.g., the robot can carry only one item at a time), this program admits optimal behavior—it describes how to deliver an item, leaving the robot to decide only on the order of deliveries. But even in settings where simultaneous or interleaved deliveries are feasible, the “nonoverlapping” program may have sufficiently high utility that restricting the robot’s choices is acceptable (since it allows the MDP to be solved much more quickly).
  </subsection>
  <subsection id="2606.6.14">
   These experiments illustrate the benefits of integrating programming and planning for mobile robot programming.
  </subsection>
  <subsection id="2606.6.15">
   We conjecture that the advantage of our framework becomes even more evident as we scale up to more complex tasks.
  </subsection>
  <subsection id="2606.6.16">
   For example, consider a robot that serves dozens of people, while making decisions as to when to recharge its batteries.
  </subsection>
  <subsection id="2606.6.17">
   Mail and coffee requests might arrive sporadically at random points in time, not just once a day (as is the case for our current implementation). Even with today’s best planners, the complexity of such tasks is well beyond what can be tackled in reasonable time. DTGolog is powerful enough to accommodate such scenarios. If supplied with programs of the type described above, we expect DTGolog to make the (remaining) planning problem tractable—with minimal effort on the programmer’s side.
  </subsection>
 </paragraph>
 <paragraph id="2606.7" name="6 Concluding Remarks">
  <subsection id="2606.7.1">
   We have provided a general first-order language for specifying MDPs and imposing constraints on the space of allowable policies by writing a program. In this way we have provided a natural framework for combining decision-theoretic planning and agent programming with an intuitive semantics. We have found this framework to be very flexible as a robot programming tool, integrating programmingand planningseamlessly and permitting thedeveloper to choose the point on this spectrum best-suited to the task at hand. While Golog has proven to be an ideal vehicle for this combination, our ideas transcend the specific choice of language.
  </subsection>
  <subsection id="2606.7.2">
   A number of interesting directions remain to be explored.
  </subsection>
  <subsection id="2606.7.3">
   The decision-tree algorithm used by the DTGolog interpreter is clearly subject to computational limitations. However, the basic intuitions and foundations of DTGolog are not wedded to this particular computational model. We are currently integrating integrating efficient algorithms and other techniques for solving MDPs into this framework (dynamic programming, abstraction, sampling, etc.). We emphasize that  Note, however, that program constraints often make otherwise intractable MDPs reasonably easy to solve using search methods.
  </subsection>
  <subsection id="2606.7.4">
   even with these methods, the ability to naturally constrain the search for good policies with explicit programs is crucial. Other avenues include: incorporating realistic models of partial observability (a key to ensuring wider applicability of the model); extending the expressive power of the language to include other extensions already defined for the classical Golog model (e.g., concurrency); incorporating declaratively-specified heuristic and search control information; monitoring of on-line execution of DTGolog programs [17]; and automatically generating sense conditions for stochastic actions.
  </subsection>
 </paragraph>
 <paragraph id="2606.8" name="References">
  <subsection id="2606.8.1">
   [1] F. Bacchus, J. Halpern, and H. Levesque. Reasoning about noisy sensors in the situation calculus. IJCAI-95, pp.1933– 1940, Montreal, 1995.
  </subsection>
  <subsection id="2606.8.2">
   [2] F. Bacchus and F. Kabanza. Using temporal logic to control search in a forward chaining planner. In M. Ghallab, A. Milani, eds., New Directions in Planning,pp.141–153, 1996.IOS Press.
  </subsection>
  <subsection id="2606.8.3">
   [3] A. Barto, S. Bradtke, and S. Singh. Learning to act using realtime dynamic programming. Art. Intel., 72:81–138, 1995.
  </subsection>
  <subsection id="2606.8.4">
   [4] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and computational leverage. J.
  </subsection>
  <subsection id="2606.8.5">
   Art. Intel. Res., 11:1–94, 1999.
  </subsection>
  <subsection id="2606.8.6">
   [5] W. Burgard, A. Cremers, D. Fox, D. Hähnel, G. Lakemeyer, D. Schulz, W. Steiner, and S. Thrun. Experiences with an interactive museum tour-guide robot. Art. Intel., 114, 1999.
  </subsection>
  <subsection id="2606.8.7">
   [6] R. Dearden and C. Boutilier. Abstraction and approximate decision theoretic planning. Art. Intel., 89:219–283, 1997.
  </subsection>
  <subsection id="2606.8.8">
   [7] H. Geffner and B. Bonet. High-levelplanning and control with incomplete information using POMDPs. AAAI Fall Symp. on Cognitive Robotics, Orlando, 1998.
  </subsection>
  <subsection id="2606.8.9">
   [8] M. Kearns, Y. Mansour, and A. Ng. A sparse sampling algorithm for near-optimal planning in large Markov decision processes. IJCAI-99, Stockholm, 1999.
  </subsection>
  <subsection id="2606.8.10">
   [9] S. Koenig and R. Simmons. Real-time search in nondeterministic domains. IJCAI-95, pp.1660–1667, Montreal, 1995.
  </subsection>
  <subsection id="2606.8.11">
   [10] H. Levesque, R. Reiter, Y. Lespérance, F. Lin, and R. Scherl.
  </subsection>
  <subsection id="2606.8.12">
   GOLOG: a logic programming language for dynamic domains. J. Logic Prog., 31(1-3):59–83, 1997.
  </subsection>
  <subsection id="2606.8.13">
   [11] R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. NIPS-10, pp.1043–1049. MIT Press. 1998.
  </subsection>
  <subsection id="2606.8.14">
   [12] D. Poole. The independent choice logic for modelling multiple agents under uncertainty. Art. Intel., 94:7–56, 1997.
  </subsection>
  <subsection id="2606.8.15">
   [13] M. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley, New York, 1994.
  </subsection>
  <subsection id="2606.8.16">
   [14] R. Reiter. Natural actions, concurrency and continuous time in the situation calculus. KR’96, pp.2–13, Cambridge, 1996.
  </subsection>
  <subsection id="2606.8.17">
   [15] R. Reiter. Sequential, temporal GOLOG. KR’98, pp.547–556, Trento, 1998.
  </subsection>
  <subsection id="2606.8.18">
   [16] R. Reiter. The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. In V. Lifschitz, ed, Artificial Intelligence and Mathematical Theory of Computation (Papers in Honor of John McCarthy), pp.359–380. Academic Press, 1991.
  </subsection>
  <subsection id="2606.8.19">
   [17] M. Soutchanski. Execution monitoring of high–level temporal programs. IJCAI-99 Workshop on Robot Action Planning, Stockholm, 1999.
  </subsection>
  <subsection id="2606.8.20">
   [18] R. Sutton. TD models: Modeling the world at a mixture of time scales. ICML-95, pp.531–539, Lake Tahoe, 1995.
  </subsection>
  <subsection id="2606.8.21">
   [19] S. Thrun, M. Bennewitz, W. Burgard, A. Cremers, F. Dellaert, D. Fox, D. Hähnel, C. Rosenberg, N. Roy, J. Schulte, and D. Schulz. MINERVA: A second generation mobile tourguide robot. ICRA-99, 1999.
  </subsection>
 </paragraph>
</paper>