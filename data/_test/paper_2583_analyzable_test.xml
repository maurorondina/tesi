<?xml version="1.0" encoding="utf-8"?>
<paper id="2583" url="http://www.aaai.org/Papers/AAAI/2000/AAAI00-128.pdf">
 <title id="2583.0" name="Active Audition for Humanoid"/>
 <subsection id="2583.0.1">
  Kazuhiro Nakadaiy , Tino Lourensy , Hiroshi G. Okunoy3 , and Hiroaki Kitanoyz yKitano Symbiotic Systems Project, ERATO, Japan Science and Technology Corp.
 </subsection>
 <subsection id="2583.0.2">
  Mansion 31 Suite 6A, 6-31-15 Jingumae, Shibuya-ku, Tokyo 150-0001, Japan Tel: +81-3-5468-1661, Fax: +81-3-5468-1664 * Department of Information Sciences, Science University of Tokyo zSony Computer Science Laboratories, Inc.
 </subsection>
 <subsection id="2583.0.3">
  fnakadai, tinog@symbio.jst.go.jp, okuno@nue.org, kitano@csl.sony.co.jp
 </subsection>
 <paragraph id="2583.1" name="Abstract">
  <subsection id="2583.1.1">
   In this paper, we present an active audition system for humanoid robot “SIG the humanoid”. The audition system of the highly intelligent humanoid requires localization of sound sources and identification of meanings of the sound in the auditory scene. The active audition reported in this paper focuses on improved sound source tracking by integrating audition, vision, and motor movements. Given the multiple sound sources in the auditory scene, SIG actively moves its head to improve localization by aligning microphones orthogonal to the sound source and by capturing the possible sound sources by vision. However, such an active head movement inevitably creates motor noise. The system must adaptively cancel motor noise using motor control signals.
  </subsection>
  <subsection id="2583.1.2">
   The experimental result demonstrates that the active audition by integration of audition, vision, and motor control enables sound source tracking in variety of conditions.
  </subsection>
 </paragraph>
 <paragraph id="2583.2" name="Introduction">
  <subsection id="2583.2.1">
   The goal of the research reported in this paper is to establish a technique of multi-modal integration for improving perception capabilities. We use an upper-torso humanoid robot as a platform of the research, because we believe that multi-modality of perception and high degree-of-freedom is essential to simulate intelligent behavior. Among various perception channels, this paper reports active audition that integrates audition with vision and motor control.
  </subsection>
  <subsection id="2583.2.2">
   Active perception is an important research topic that signifies coupling of perception and behavior. A lot of research has been carried out in the area of active vision, because it will provide a framework for obtaining necessary additional information by coupling vision with behaviors, such as control of optical parameters or actuating camera mount positions. For example, an observer controls the geometry parameters of the sensory apparatus in order to improve the quality of the perceptual processing (Aloimonos, Weiss, &amp; Bandyopadhyay. 1987). Such activities include moving a camera or cameras (vergence), changing focus, zooming in or out, changing camera resolution, widening or narrowing iris and so on. Therefore, active vision system is always Copyright c 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.
  </subsection>
  <subsection id="2583.2.3">
   coupled with servo-motor system, which means that active vision system is in general associated with motor noise.
  </subsection>
  <subsection id="2583.2.4">
   The concept of active perception can be extended to audition, too. Audition is always active, since people hear a mixture of sounds and focus on some parts of input. Usually, people with normal hearing can separate sounds from a mixture of sounds and focus on a particular voice or sound even in a noisy environment. This capability is known as the cocktail party effect. While traditionally, auditory research has been focusing on human speech understanding, understanding auditory scene in general is receiving increasing attention. Computational Auditory Scene Analysis (CASA) studies a general framework of sound processing and understanding (Brown 1992; Cooke et al. 1993; Nakatani, Okuno, &amp; Kawabata 1994; Rosenthal &amp; Okuno 1998). Its goal is to understand an arbitrary sound mixture including speech, non-speech sounds, and music in various acoustic environment. It requires not only understanding of meaning of specific sound, but also identification of spatial relationship of sound sources, so that sound landscapes of the environment can be understood. This leads to the need of active audition that has capability of dynamically focusing on specific sound in a mixture of sounds, and actively controlling motor systems to obtain further information using audition, vision, and other perceptions.
  </subsection>
  <subsection id="2583.2.5">
   Audition for Humanoids in Daily Environments Our ultimate goal is to deploy our robot in daily environments. For audition, this requires the following issues to be resolved:  Ability to localize sound sources in unknown acoustic environment.
  </subsection>
  <subsection id="2583.2.6">
   Ability to actively move its body to obtain further information from audition, vision, and other perceptions.
  </subsection>
  <subsection id="2583.2.7">
   Ability to continuously perform auditory scene analysis under noisy environment, where noise comes from both environment and motor noise of robot itself.
  </subsection>
  <subsection id="2583.2.8">
   First of all, deployment to the real world means that the acoustic features of the environment is not known in advance. In the current computational audition model, the Head-Related Transfer Function (HRTF) was measured in From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved.
  </subsection>
  <subsection id="2583.2.9">
   a) Cover Motor1 Motor2 Motor3 (occluded) Motor4 Camera Motor (Left Tilt) Camera Motor (Right Tilt) Camera Motor (Right Pan) Camera Motor (Left Pan) b) Mechanical structure Internal Microphones External Microphones c) Internal microphones (top) and cameras Figure 1: SIG the Humanoid the specific room environment, and measurement has to be repeated if the system is installed at different room. It is infeasible for any practical system to require such extensive measurement of the operating space. Thus, audition system without HRTF is an essential requirement for practical systems. The system reported in thispaper implements epipolar geometry-based sound source localization that eliminates the need for HRTF. The use of epipolar geometry for audition is advantageous when combined with the vision system because many vision systems uses epipolar geometry for visual object localization.
  </subsection>
  <subsection id="2583.2.10">
   Second, active audition that couples audition, vision, and motor control system is critical. Active audition can be implemented in various aspects. Take the most visible example, the system should be able to dynamically align microphone positions against sound sources to obtain better resolution. Consider that a humanoid has a pair of microphones. Given the multiple sound sources in the auditory scene, the humanoid should actively move its head to improve localization (getting the direction of a sound source) by aligning microphones orthogonal to the sound source. Aligning a pair of microphones orthogonal to the sound source has several advantages:  Each channel receives the sound from the sound source at the same time.
  </subsection>
  <subsection id="2583.2.11">
   It is rather easy to extract sounds originating from the center by comparing subbands in each channel.
  </subsection>
  <subsection id="2583.2.12">
   The problem of front-behind sound from such sound source can be solved by using direction-sensitive microphones.
  </subsection>
  <subsection id="2583.2.13">
   The sensitivity of direction in processing sounds is expected to be higher along the center line, because sound is represented by a sine function.
  </subsection>
  <subsection id="2583.2.14">
   Zooming of audition can be implemented by using nondirectional and direction-sensitive microphones.
  </subsection>
  <subsection id="2583.2.15">
   Therefore, gaze stabilization for microphones is very important to keep the same position relative to a target sound source.
  </subsection>
  <subsection id="2583.2.16">
   Active audition requires movement of the components that mounts microphone units. In many cases, such a mount is actuated by motors that create considerable noise. In a complex robotic system, such as humanoid, motor noise is complex and often irregular because numbers of motors may be involved in the head and body movement. Removing motor noise from auditory system requires information on what kind of movement the robot is making in realtime. In other words, motor control signals need to be integrated as one of the perception channels. If dynamic noise canceling of motor noise fails, one may end-up using “stop-perceive-act” principle reluctantly, so that the audition system can receive sound without motor noise. To avoid using such an implementation, we implemented an adaptive noise canceling scheme that uses motor control signal to anticipate and cancel motor noise.
  </subsection>
  <subsection id="2583.2.17">
   For humanoid audition, active audition and the CASA approach is essential. In this paper, we investigate a new sound processing algorithm based on epipilar geometry without using HRTF, and internal sound suppression algorithms.
  </subsection>
  <subsection id="2583.2.18">
   SIG the humanoid As a testbed of integration of perceptual information to control motor of high degree of freedom (DOF), we designed a humanoid robot (hereafter, referred as SIG) with the following components (Kitano et al. 2000):  4 DOFs of body driven by 4 DC motors — Its mechanical structure is shown in Figure 1b. Each DC motor is controlled by a potentiometer.
  </subsection>
  <subsection id="2583.2.19">
   A pair of CCD cameras of Sony EVI-G20 for visual stereo input — Each camera has 3 DOFs, that is, pan, tilt  and zoom. Focus is automatically adjusted. The offset of camera position can be obtained from each camera (Figure 1b).
  </subsection>
  <subsection id="2583.2.20">
   Two pairs of nondirectional microphones (Sony ECM77S) (Figure 1c). One pair of microphones are installed at the ear position of the head to gather sounds from the external world. Each microphone is shielded by the cover to prevent from capturing internal noises. The other pair of microphones are installed very close to the corresponding microphone to gather sounds from the internal world.
  </subsection>
  <subsection id="2583.2.21">
   A cover of the body (Figure 1a) reduces sounds to be emitted to external environments, which is expected to reduce the complexity of sound processing.
  </subsection>
 </paragraph>
 <paragraph id="2583.3" name="New Issues of Humanoid Audition">
  <subsection id="2583.3.1">
   This section describes our motivation of humanoid audition and some related work. We assume that a humanoid or robot will move even while it is listening to some sounds. Most robots equipped with microphones developed so far process sounds without motion (Huang, Ohnishi,  Sugie 1997; Matsusaka et al. 1999; Takanishi et al. 1995). This “stop-perceive-act” strategy, or hearing without movements, should be conquered for real-world applications.
  </subsection>
  <subsection id="2583.3.2">
   For this purpose, hearing with robot movements imposes us various new and interesting aspects of existing problems.
  </subsection>
  <subsection id="2583.3.3">
   The main problems with humanoid audition during motion includes understanding general sounds, sensor fusion, active audition, and internal sound suppression.
  </subsection>
  <subsection id="2583.3.4">
   General Sound Understanding Since computational auditory scene analysis (CASA) research investigates a general model of sound understanding, input sound is a mixture of sounds, not a sound of single source. One of the main research topics of CASA is sound stream separation, a process that separates sound streams that have consistent acoustic attributes from a mixture of sounds. Three main issues in sound stream separation are 1. Acoustic features used as clues of separation, 2. Real-time and incremental separation, and 3. Information fusion — discussed separately.
  </subsection>
  <subsection id="2583.3.5">
   Inextractingacoustic attributes, some systems assumethe humans auditory model of primary processing and simulate theprocessing of cocklear mechanism (Brown 1992; Slaney, Naar,  Lyon 1994). Brown and Cooke designed and implemented a system that builds various auditory maps for sound input and integrates them to separate speech from input sounds (Brown 1992).
  </subsection>
  <subsection id="2583.3.6">
   Nakatani, Okuno,  Kawabata 1994 used harmonic structures as the clue of separation and developed a monaural-based harmonic stream separation system, called HBSS. HBSS is modeled by a multi-agent system and extracts harmonic structures incrementally. They extended HBSS to use binaural (stereo microphone embedded in a dummy head) sounds and developed a binaural-based harmonicstream separation system, called Bi-HBSS (Nakatani, Okuno,  Kawabata 1995). Bi-HBSS uses harmonic structures and the direction of sound sources as clues of separation. Okuno, Nakatani,  Kawabata 1999 extended Bi-HBSS to separate speech streams, and uses the resulting system as a front end for automatic speech recognition.
  </subsection>
  <subsection id="2583.3.7">
   Sensor Fusion for Sound Stream Separation Separation of sound streams from perceptive input is a nontrivial task due to ambiguities of interpretation on which elements of perceptive input belong to which stream (Nakagawa, Okuno,  Kitano 1999). For example, when two independent sound sources generate two sound streams that are crossing in the frequency region, there may be two possibilities; crossing each other, or approaching and departing.
  </subsection>
  <subsection id="2583.3.8">
   The key idea of Bi-HBSS is to exploit spatial information by using a binaural input.
  </subsection>
  <subsection id="2583.3.9">
   Staying within a single modality, it is very difficult to attain high performance of sound stream separation. For example, Bi-HBSS finds a pair of harmonic structures extracted by left and right channels similar to stereo matching in vision where camera are aligned on a rig, and calculates the interaural time/phase difference (ITD or IPD), and/or the interaural intensity/amplitude difference (IID or IAD) to obtain the direction of sound source. The mapping from ITD, IPD, IID and IAD to the direction of sound source and vice versa is based on the HRTF associated to binaural microphones. Finally Bi-HBSS separates sound streams by using harmonic structure and sound source direction.
  </subsection>
  <subsection id="2583.3.10">
   The error in direction determined by Bi-HBSS is about 610, which is similar to that of a human, i.e. 68 (Cavaco 1999). However, this is too coarse to separate sound streams from a mixture of sounds.
  </subsection>
  <subsection id="2583.3.11">
   Nakagawa, Okuno,  Kitano 1999 improved the accuracy of the sound source direction by using the direction extracted by image processing, because the direction by vision is more accurate. By using an accurate direction, each sound stream is extracted by using a direction-pass filter. In fact, by integrating visual and auditory information, they succeeded to separate three sound sources from a mixture of sounds by two microphones. They also reported how the accuracy of sound stream separation measured by automatic speech recognition is improved by adding more modalities, from monaural input, binaural input, and binaural input with visual information.
  </subsection>
  <subsection id="2583.3.12">
   Some critical problems with Bi-HBSS and their work for real-world applications are summarized as follows: 1. HRTF is needed for identifying the direction. It is timeconsuming to measure HRTF, and it is usually measured in an aechotic room. Since it depends on auditory environments, re-measurement or adaptation is needed to apply it to other environments.
  </subsection>
  <subsection id="2583.3.13">
   2. HRTF is needed for creating a direction-pass filter.
  </subsection>
  <subsection id="2583.3.14">
   Their direction-pass filter needs HRTF to compose. Since HRTF is usually measured in discrete azimuth and elevation, it is difficult to implement sound tracking for continuous movement of sound sources.
  </subsection>
  <subsection id="2583.3.15">
   Therefore, a new method without using HRTF should be invented for localization (sound source direction) and  direction (by using a direction-pass filter). We will propose a new auditory localization based on the epipolar geometry.
  </subsection>
  <subsection id="2583.3.16">
   Sound Source Localization Some robots developed so far had a capability of sound source localization. Huang, Ohnishi,  Sugie 1997 developed a robot that had three microphones. Three microphones were installed vertically on the top of the robot, composing a triangule. Comparing the input power of microphones, two microphones that have more power than the other are selected and the sound source direction is calculated. By selecting two microphones from three, they solved the problem that two microphones cannot determine the place of sound source in front or backward. By identifying the direction of sound source from a mixture of an original sound and its echoes, the robot turns the body towards the sound source.
  </subsection>
  <subsection id="2583.3.17">
   Humanoids of Waseda University can localize a sound source by using two microphones (Matsusaka et al. 1999; Takanishi et al. 1995). These humanoids localize a sound source by calculating IID or IPD with HRTF. These robot can neither separate even a sound stream nor localize more than one sound source. The Cog humanoid of MIT has a pair of omni-directional microphones embedded in simplified pinnae (Brooks et al. 1999a; Irie 1997). In the Cog, auditory localization is trained by visual information.
  </subsection>
  <subsection id="2583.3.18">
   This approach does not use HRTF, but assumes a single sound source. To summarize, both approaches lack for the CASA viewpoints.
  </subsection>
  <subsection id="2583.3.19">
   Active Audition A humanoid is active in the sense that it tries to do some activity to improve perceptual processing. Such activity includes to change the position of cameras and microphones by motor control.
  </subsection>
  <subsection id="2583.3.20">
   When a humanoid hears sound by facing the sound source in the center of the pair of microphones, ITD and IID is almost zero if the pair of microphones are correctly calibrated. In addition, sound intensity of both channels becomes stronger, because the ear cover makes a nondirectional microphone directional. Given the multiple sound sources in the auditory scene, a humanoid actively moves its head to improve localization by aligning microphones orthogonal to the sound source and by capturing the possible sound sources by vision.
  </subsection>
  <subsection id="2583.3.21">
   However, a new problem occurs because gaze stabilization is attained by visual servo or auditory servo. Sounds are generated by motor rotation, gears, belts and ball bearings. Since these internal sound sources are much closer than other external sources, even if the absolute power of sounds is much lower, input sounds are strongly influenced.
  </subsection>
  <subsection id="2583.3.22">
   This is also the case for the SONY AIBO entertainment robot; AIBO is equipped with a microphone, but internal noise mainly caused by a cooling fan is too large to utilize sounds.
  </subsection>
  <subsection id="2583.3.23">
   cover pan-tilt-zoom camera suppress suppress internal microphone external microphone Figure 2: Internal and external microphones for internal sound suppression Internal Sound Suppression Since active perception causes sounds by the movement of various movable parts, internal sound suppression is critical to enhance external sounds (see Figure 2). A cover of humanoid body reduces sounds of motors emitted to the external world by separating internal and external world of the robot. Such a cover is, thus expected to reduce the complexity of sound processing caused by motor sounds.
  </subsection>
  <subsection id="2583.3.24">
   Since most robots developed so far do not have a cover, auditory processing cannot become first-class perception of a humanoid.
  </subsection>
  <subsection id="2583.3.25">
   Internal sound suppression may be attained by one or a combination of the following methodologies: 1. noise cancellation, 2. independent component analysis (ICA), 3. case-based suppression, 4. model-based suppression, and 5. learning and adaptation.
  </subsection>
  <subsection id="2583.3.26">
   To record sounds for case-based suppression and modelbased suppression, each sound should be labeled appropriately. We use data consisting of time and motor control commands as label for sound. In the next section, we will explain how these methods are utilized in our active audition system.
  </subsection>
 </paragraph>
 <paragraph id="2583.4" name="Active Audition System" label="Problem">
  <subsection id="2583.4.1">
   An active audition system consists of two components; internal sound suppression, and sound stream separation.
  </subsection>
  <subsection id="2583.4.2">
   Internal Sound Suppression System Internal sounds of SIG are caused mainly by the followings:  Camera motors — sounds of movement are quiet enough to ignore, but sounds of standby is loud (about 3.7 dB).
  </subsection>
  <subsection id="2583.4.3">
   Body motors — sounds of standby and movement are loud (about 5.6 dB and 23 dB, respectively).
  </subsection>
  <subsection id="2583.4.4">
   Comparison of noise cancellation by adaptive filtering, ICA, case-based suppression and model-based suppression, we concluded that only adaptive filters work well. Four microphones are not enough for ICA to separate internal sounds. Case-based and model-based suppression affect the phase of original inputs, which causes errors of IPD.
  </subsection>
  <subsection id="2583.4.5">
   Our adaptive filter uses heuristics with internal microphones, which specifies the condition to cut off burst noise  mainly caused by motors. For example, sounds at stoppers, by friction between cable and body, creaks at joints of cover parts may occur. The heuristics orders that localization by sound or direction-pass filter ignore a subband if the following conditions hold: 1. The power of internal sounds is much stronger than that of external sounds.
  </subsection>
  <subsection id="2583.4.6">
   2. Twenty adjacent subbands have strong power (30 dB).
  </subsection>
  <subsection id="2583.4.7">
   3. A motor motion is being processed.
  </subsection>
  <subsection id="2583.4.8">
   We tried to make as adaptive filter an FIR (Finite Impulse Response) filter of order 100, because this filter is a linear phase filter. This property is essential to localize the sound source by IID (Interaural Intensity Difference) or ITD/IPD (Interaural Time/Phase Difference). The parameters of the FIR filter is calculated by least-mean-square method as adaptive algorithm. Noise cancellation by the FIR filter suppresses internal sounds but some errors occur.
  </subsection>
  <subsection id="2583.4.9">
   These errors make poor localization compared to results of localization without internal sound suppression. Casebased or model-based cancellation is not adopted, because the same movement generates a lot of different sounds and thus it is difficult to construct case or model-based cancellation.
  </subsection>
  <subsection id="2583.4.10">
   Instead, internal sound suppression system consists of the following subcomponents: 1. Filtering by threshold — Since standby sounds of camera motor are stable and limited in frequency range, that is, at frequencies of less than 200 Hz, we confirmed that the filtering of weak sounds less than the threshold is effective.
  </subsection>
  <subsection id="2583.4.11">
   2. Adaptive filter — Since suppression of sounds affects phase information, we design a new adaptive filter that switches through or cut whether the power of internal microphone is stronger than that of an external microphone. If this condition holds, the system assumes that internal sounds are generated.
  </subsection>
  <subsection id="2583.4.12">
   Sound Stream Separation by Localization We design a new direction-pass filter with a direction which is calculated by epipolar geometry.
  </subsection>
  <subsection id="2583.4.13">
   Localization by Vision using Epipolar Geometry Consider a simple stereo camera setting where two cameras have the same focal length, their light axes are in parallel, and their image planes are on the same plane (see Figure 3a).
  </subsection>
  <subsection id="2583.4.14">
   We define the world coordinate (X; Y; Z) and each local coordinate. Suppose that a space point P(X; Y; Z) is projected on each camera’s image plane, (xl; yl) and (xr; yr).
  </subsection>
  <subsection id="2583.4.15">
   The following relations hold (Faugeras 1993): X = b(xl + xr) 2d ; Y = b(yl + yr) 2d ; Z = bf d where f is the focal length of each camera’s lens and b is the baseline. Disparity d is defined as d = xl 0xr.
  </subsection>
  <subsection id="2583.4.16">
   The current implementation of common matching in SIG is performed by using corner detection algorithm (Lourens et al. 2000). It extracts a set of corners and edges then θ baseline b P( X, Y, Z ) Cr Cl space point f f X Y Z Mr Ml baseline b l l l a) Vision b) Audition Pr ( xr , yr ) Pl ( xl , yl ) P( ) θ Cl, Cr: camera center, Ml, Mr: microphone center Figure 3: Epipolar geometry for localization constructs a pair of graphs. A matching algorithm is used to find corresponding left and right image to obtain depth.
  </subsection>
  <subsection id="2583.4.17">
   Since the relation yl = yr also holds under the above setting, a pair of matching points in each image plane can be easily sought. However, for general setting of camera positions, matching is much more difficult and timeconsuming. Usually, a matching point in the other image plane exists on the epipolar line which is a bisecting line made by the epipolar plane and the image plane.
  </subsection>
  <subsection id="2583.4.18">
   Localization by Audition using Epipolar Geometry Auditory system extracts the direction by using epipolar geometry. First, it extract peaks by using FFT (Fast Fourier Transformation) for each subband, 47Hz in our implementation, and then calculates the IPD.
  </subsection>
  <subsection id="2583.4.19">
   Let Sp(r) and Sp(l) be the right and left channel spectrum obtained by FFT at the same time tick. Then, the IPD 4' is calculated as follows: 4' = tan01 =[Sp(r) (fp)] [Sp(r)(fp)]  0tan01 =[Sp(l) (fp)] [Sp(l)(fp)]  where fp is a peak frequency on the spectrum, [Sp] and =[Sp] are the real and imaginary part of the spectrum Sp(r) .
  </subsection>
  <subsection id="2583.4.20">
   The angle  is calculated by the following equation: cos  = v 2fpb 4' where v is the velocity of sound. For the moment, the velocity of sound is fixed to 340m/sec and remains the same even if the temperature changes.
  </subsection>
  <subsection id="2583.4.21">
   This peak extraction method works at 48 KHz sampling rate and calculates FFT for 1,024 points, but runs much faster than Bi-HBSS (12 KHz sampling rate with HRTF) and extracted peaks are more accurate (Nakadai, Okuno,  Kitano 1999).
  </subsection>
  <subsection id="2583.4.22">
   New Direction-Pass Filter using Epipolar Geometry As mentioned earlier, HRTF is usually not available in real-world environments, because it changes when a new furniture is installed, a new object comes in the room, or humidity of the room changes. In addition, HRTF should  Image Understanding Association Sound Understanding Motor Control Sound Stream Separation Focus of Attention Action Selection features features direction pitch direction speed Localization by Vision Localization by Audition Localization by Actuator features Figure 4: Integrated humanoid perception system be interpolated for auditory localization of a moving sound source, because HRTF is measured for discrete positions.
  </subsection>
  <subsection id="2583.4.23">
   Therefore, a new method must be invented. Our method is based on the direction-pass filter with epipolar geometry.
  </subsection>
  <subsection id="2583.4.24">
   As opposed to localization by audition, the direction-pass filter selects subbands that satisfies the IPD of the specified direction. The detailed algorithm is describes as follows: 1. The specified direction  is converted to 4' for each subband (47 Hz).
  </subsection>
  <subsection id="2583.4.25">
   2. Extract peaks and calculated IPD, 4'0.
  </subsection>
  <subsection id="2583.4.26">
   3. If IPD satisfies the specified condition, namely, 4'0 = 4', then collect the subband.
  </subsection>
  <subsection id="2583.4.27">
   4. Construct a wave consisting of collected subbands.
  </subsection>
  <subsection id="2583.4.28">
   By using the relative position between camera centers and microphones, it is easy to convert from epipolar plane of vision to that of audition (see Figure 3b). In SIG, the baselines for vision and audition are in parallel.
  </subsection>
  <subsection id="2583.4.29">
   Therefore, whenever a sound source is localized by epipolar geometry in vision, it can be converted easily into the angle  as described in the following equation: cos  = ~ P 1 ~ Mr j~ Pjj ~ Mrj = ~ P 1 ~ Cr j~ Pjj ~ Crj : Localization by Servo-Motor System The head direction is obtained from potentiometers in the servo-motor system. Hereafter, it is referred as the head direction by motor control. Head direction by potentiometers is quite accurate by the servo-motor control mechanism. If only the horizontal rotation motor is used, horizontal direction of the head is obtained accurately, about 61. By combining visual localization and the head direction, SIG can determine the position in world coordinates.
  </subsection>
  <subsection id="2583.4.30">
   Accuracy of Localization Accuracy of extracted directions by three sensors: vision, audition, and motor control is measured. The results for the current implementation are 61, 610, 615, for vision, motor control, and audition, respectively.
  </subsection>
  <subsection id="2583.4.31">
   Therefore, the precedence of information fusion on direction is determined as below: vision  motor control  audition Sensor Integrated System The system contains a perception system that integrates sound, vision, and motor control (Figure 4). The association module maintains the consistency between information extracted by image processing, sound processing and motor control subsystems. For the moment, association includes the correspondence between images and sounds for a sound source; loud speakers are the only sound sources, which can generate sound of any frequency. Focus of attention and action selection modules are described in (Lourens et al. 2000).
  </subsection>
 </paragraph>
 <paragraph id="2583.5" name="Experiment — Motion Tracking by Three Kinds of Sensors">
  <subsection id="2583.5.1">
   In this section, we will demonstrate how vision, audition and head direction by potentiometers compensate each missing information to localize sound sources while SIG rotates to see an unknown object.
  </subsection>
  <subsection id="2583.5.2">
   Scenario: There are two sound sources: two BW Noutilus 805 loud speakers located in a room of 10 square meters. The room where the system is installed is a conventional residential apartment facing a road with busy traffic, and exposed to various daily life noise. The sound environment is not controlled at all for experiments to ensure feasibility of the approach in daily life.
  </subsection>
  <subsection id="2583.5.3">
   One sound source A (Speaker A) plays a monotone sound of 500 Hz. The other sound source B (Speaker B) plays a monotone sound of 600 Hz. A is located in front of SIG (5 left of the initial head direction) and B is located 69 to the left. The distance from SIG to each sound source is about 210cm. Since the visual field of camera is only 45 in horizontal angle, SIG cannot see B at the initial head direction, because B is located at 70 left to the head direction, thus it is outside of the visual fields of the cameras. Figure 5 shows this situation.
  </subsection>
  <subsection id="2583.5.4">
   1. A plays a sound at 5 left of the initial head direction.
  </subsection>
  <subsection id="2583.5.5">
   2. SIG associates the visual object with the sound, because their extracted directions are the same.
  </subsection>
  <subsection id="2583.5.6">
   3. Then, B plays a sound about 3 seconds later. At this moment, B is outside of the visual field of the SIG. Since the direction of the sound source can be extracted only by audition, SIG cannot associate anything to the sound.
  </subsection>
  <subsection id="2583.5.7">
   4. SIG turns toward the direction of the unseen sound source B using the direction obtained by audition.
  </subsection>
  <subsection id="2583.5.8">
   5. SIG finds a new object B, and associates the visual object with the sound.
  </subsection>
  <subsection id="2583.5.9">
   Four kinds of benchmark sounds are examined; fast (68.8 degree/sec) and slow (14.9 degree/sec) movement of SIG.
  </subsection>
  <subsection id="2583.5.10">
   Weak signals (similar power to internal standby sounds, which makes signal to noise ratio 0dB) and strong signals (about 50 dB). Spectrogram of each input is shown in Figure 6. Motion tracking by vision and audition, and motion information are evaluated.
  </subsection>
  <subsection id="2583.5.11">
   Results: Results of the experiment were very promising.
  </subsection>
  <subsection id="2583.5.12">
   First, accurate sound source localization was accomplished without using the HRTF. The use of epipolar geometry for  Humanoid initial direction Loud Speaker A (500Hz) Loud Speaker B (600Hz) 0 o 90 o 180 o 53 o 58 o 127 o 131 o rotation range 81 o 104 o Both speakers are out of sight final direction Figure 5: Experiment: Motion tracking by vision and audition while SIG moves.
  </subsection>
  <subsection id="2583.5.13">
   0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 11 0 200 400 600 800 1000 1200 1400 1600 1800 2000 a) fast movement of SIG b) slow movement of SIG Figure 6: Spectrogram of input sounds 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 -150 -100 -50 0 50 100 150 Time (sec) Direction in humanoid coordinate (degree) ↓ A: Audition ↑ B: Audition 0 1 2 3 4 5 6 7 8 9 10 11 -150 -100 -50 0 50 100 150 Time (sec) Direction in humanoid coordinate (degree) ↓ A: Audition ↑ B: Audition a) fast movement of SIG b) slow movement of SIG Figure 7: Localization without heuristics of suppression 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 -150 -100 -50 0 50 100 150 A disappears → ← B appears ↓ A: Audition ↑ B: Audition ↓ +Motor B: Vision ↑ A: Vision Time (sec) Direction in humanoid coordinate (degree) 0 1 2 3 4 5 6 7 8 9 10 11 -150 -100 -50 0 50 100 150 ↓ A: Audition ↑ B: Audition ↓ +Motor B:Vision ↑ A: Vision Time (sec) Direction in humanoid coordinate (degree) A disappears → ← B appears a) fast movement of SIG b) slow movement of SIG Figure 8: Localization by vision and audition 0 1 2 3 4 5 6 7 8 9 10 -80 -60 -40 -20 0 20 40 60 80 Time (sec) Direction in humanoid coordinate (degree) B: Audition A: Vision A: Audition B: Vision + Motor A disappears B appears 0 2 4 6 8 10 12 14 16 -80 -60 -40 -20 0 20 40 60 80 Time (sec) Direction in humanoid coordinate (degree) B: Audition A: Vision A: Audition B: Vision + Motor A disappears B appears a) fast movement of SIG b) slow movement of SIG Figure 9: Localization for strong signal (50dB) audition was proven to be very effective. In both cases of weak and strong sound, epipolar based non-HRTF method locate approximate direction of sound sources (see localization date for initial 5 seconds in Figure 7). In Figure 7, time series data for estimated sound source direction using only audition is plotted with an ego-centric polar coordinate where 0 is the direction dead front of the head, minus is right of the head direction.
  </subsection>
  <subsection id="2583.5.14">
   The effect of adaptive noise canceling is clearly shown.
  </subsection>
  <subsection id="2583.5.15">
   Figure 7 shows estimated sound source directions without motor noise suppression. Sound direction estimation is seriously hampered when the head is moving (around time 5 6 seconds). The spectrogram (Figure 6) clearly indicates extensive motor noise. When the robot is constantly moving to track moving sound sources or to move itself for a certain position, the robot continues to generate such a noise that makes audition almost impossible to use for perception.
  </subsection>
  <subsection id="2583.5.16">
   The effects of internal sound suppression by heuristics are shown in Figures 8, and 9. The time series of estimated sound source directions for weak and strong signals localized by vision and audition are shown.
  </subsection>
  <subsection id="2583.5.17">
   Such accurate localization by audition makes association between audition and vision possible. While SIG is moving, sound source B comes into its visual field. The association module checks the consistency of localization by vision and audition. If the discovered loud speaker does not play sounds, inconsistency occurs and the visual system would resume its search finding an object producing sound. If association succeeds, B’s position in world coordinates is calculated by using motor information and the position in humanoid coordinates obtained by vision.
  </subsection>
  <subsection id="2583.5.18">
   Experimental results indicate that position estimation by audition and vision is accurate enough to create consistent association even under the condition that the robot is constantly moving and generating motor noise. It should be refined that sound source localization by audition in the experiment uses epipolar geometry for audition, and do not use HRTF. Thus, we can simply field the robot in unknown acoustic environment and localize sound sources.
  </subsection>
 </paragraph>
 <paragraph id="2583.6" name="Discussion and Future Work">
  <subsection id="2583.6.1">
   1. The experiment demonstrates the feasibility of the proposed humanoid audition in real-world environments.
  </subsection>
  <subsection id="2583.6.2">
   Since there are a lot of non-desired sounds, caused by traffic, people outside the test-room, and of course internal sounds, the CASA assumption that input sounds consist of a mixture of sounds is essential in real-world environments. Similar work by Nakagawa, Okuno,  Kitano 1999 was done in a simulated acoustic environment, but it may fail in localization and sound stream separation in real-world environments. Most robots capable of auditory localization developed so far assume a single sound source.
  </subsection>
  <subsection id="2583.6.3">
   2. Epipolar geometry gives a way to unify visual and auditory processing, in particular localization and sound stream separation. This approach can dispense with HRTF. As far as we know, no other systems can do it.
  </subsection>
  <subsection id="2583.6.4">
   Most robots capable of auditory localization developed  so far use HRTF explicitly or implicitly, and may fail in identifying some spatial directions or tracking moving sound sources.
  </subsection>
  <subsection id="2583.6.5">
   3. The cover of the humanoid is very important to separate its internal and external worlds. However, we’ve realized that resonance within a cover is not negligible. Therefore, its inside material design is important.
  </subsection>
  <subsection id="2583.6.6">
   4. Social interaction realized by utilizing body movements extensively makes auditory processing more difficult.
  </subsection>
  <subsection id="2583.6.7">
   The Cog Project focuses on social interaction, but this influence on auditory processing has not been mentioned (Brooks et al. 1999b). A cover of the humanoid will play an important role in reducing sounds caused by motor movements emitted toward outside the body as well as in giving a friendly outlook to human.
  </subsection>
  <subsection id="2583.6.8">
   Future Work Active perception needs self recognition.
  </subsection>
  <subsection id="2583.6.9">
   The problem of acquiring the concept of self recognition in robotics has been pointed out by many people. For audition, handling of internal sounds made by itself is a research area of modeling of self. Other future work includes more tests for feasibility and robustness, real-time processing of vision and auditory processing, internal sound suppression by independent component analysis, addition of more sensor information, and applications.
  </subsection>
 </paragraph>
 <paragraph id="2583.7" name="Conclusion">
  <subsection id="2583.7.1">
   In this paper, we present active audition for humanoid which includes internal sound suppression, a new method for auditory localization, and a new method for separating sound sources from a mixture of sounds. The key idea is to use epipolar geometry to calculate the sound source direction and to integrate vision and audition in localization and sound stream separation. This method does not use HRTF (Head-Related Transfer Function) which is a main obstacle in applying auditory processing to real-world environments.
  </subsection>
  <subsection id="2583.7.2">
   We demonstrate the feasibility of motion tracking by integrating vision, audition and motion information. The important research topic now is to explore possible interaction of multiple sensory inputs which affects quality (accuracy, computational costs, etc) of the process, and to identify fundamental principles for intelligence.
  </subsection>
 </paragraph>
 <paragraph id="2583.8" name="Acknowledgments">
  <subsection id="2583.8.1">
   We thank our colleagues of Symbiotic Intelligence Group, Kitano Symbiotic Systems Project; Yukiko Nakagawa, Dr. Iris Fermin, and Dr. Theo Sabish for their discussions. We thank Prof. Hiroshi Ishiguruo of Wakayama University for his help in active vision and integration of visual and auditory processing.
  </subsection>
 </paragraph>
 <paragraph id="2583.9" name="References">
  <subsection id="2583.9.1">
   Aloimonos, Y.; Weiss, I.; and Bandyopadhyay., A. 1987. Active vision. International Journal of Computer Vision 1(4):333–356.
  </subsection>
  <subsection id="2583.9.2">
   Brooks, R.; Breazeal, C.; Marjanovie, M.; Scassellati, B.; and Williamson, M. 1999a. The cog project: Building a humanoid robot. Technical report, MIT.
  </subsection>
  <subsection id="2583.9.3">
   Brooks, R.; Breazeal, C.; Marjanovie, M.; Scassellati, B.; and Williamson, M. 1999b. The cog project: Building a humanoid robot. In Lecture Notes in Computer Science, to appear.
  </subsection>
  <subsection id="2583.9.4">
   Spriver-Verlag.
  </subsection>
  <subsection id="2583.9.5">
   Brown, G. J. 1992. Computational auditory scene analysis: A representational approach. University of Sheffield.
  </subsection>
  <subsection id="2583.9.6">
   Cavaco, S. ad Hallam, J. 1999. A biologically plausible acoustic azimuth estimation system. In Proceedings of IJCAI-99 Workshop on Computational Auditory Scene Analysis (CASA’99), 78–87. IJCAI.
  </subsection>
  <subsection id="2583.9.7">
   Cooke, M. P.; Brown, G. J.; Crawford, M.; and Green, P. 1993.
  </subsection>
  <subsection id="2583.9.8">
   Computational auditory scene analysis: Listening to several things at once. Endeavour 17(4):186–190.
  </subsection>
  <subsection id="2583.9.9">
   Faugeras, O. D. 1993. Three Dimensional Computer Vision: A Geometric Viewpoint. MA.: The MIT Press.
  </subsection>
  <subsection id="2583.9.10">
   Huang, J.; Ohnishi, N.; and Sugie, N. 1997. Separation of multiple sound sources by using directional information of sound source. Artificial Life and Robotics 1(4):157–163.
  </subsection>
  <subsection id="2583.9.11">
   Irie, R. E. 1997. Multimodal sensory integration for localization in a humanoid robot. In Proceedings of the Second IJCAI Workshop on Computational Auditory Scene Analysis (CASA’97), 54–58. IJCAI.
  </subsection>
  <subsection id="2583.9.12">
   Kitano, H.; Okuno, H. G.; Nakadai, K.; Fermin, I.; Sabish, T.; Nakagawa, Y.; and Matsui, T. 2000. Designing a humanoid head for robocup challenge. In Proceedings of Agent 2000 (Agent 2000), to appear.
  </subsection>
  <subsection id="2583.9.13">
   Lourens, T.; Nakadai, K.; Okuno, H. G.; and Kitano, H. 2000.
  </subsection>
  <subsection id="2583.9.14">
   Selective attention by integration of vision and audition. In submitted.
  </subsection>
  <subsection id="2583.9.15">
   Matsusaka, Y.; Tojo, T.; Kuota, S.; Furukawa, K.; Tamiya, D.; Hayata, K.; Nakano, Y.; and Kobayashi, T. 1999. Multiperson conversation via multi-modal interface — a robot who communicates with multi-user. In Proceedings of Eurospeech, 1723–1726. ESCA.
  </subsection>
  <subsection id="2583.9.16">
   Nakadai, K.; Okuno, H. G.; and Kitano, H. 1999. A method of peak extraction and its evaluation for humanoid. In SIGChallenge-99-7, 53–60. JSAI.
  </subsection>
  <subsection id="2583.9.17">
   Nakagawa, Y.; Okuno, H. G.; and Kitano, H. 1999. Using vision to improve sound source separation. In Proceedings of 16th National Conference on Artificial Intelligence (AAAI-99), 768–775. AAAI.
  </subsection>
  <subsection id="2583.9.18">
   Nakatani, T.; Okuno, H. G.; and Kawabata, T. 1994. Auditory stream segregation in auditory scene analysis with a multi-agent system. In Proceedings of 12th National Conference on Artificial Intelligence (AAAI-94), 100–107. AAAI.
  </subsection>
  <subsection id="2583.9.19">
   Nakatani, T.; Okuno, H. G.; and Kawabata, T. 1995. Residuedriven architecture for computational auditory scene analysis. In Proceedings of 14th International Joint Conference on Artificial Intelligence (IJCAI-95), volume 1, 165–172. AAAI.
  </subsection>
  <subsection id="2583.9.20">
   Okuno, H. G.; Nakatani, T.; and Kawabata, T. 1999. Listening to two simultaneous speeches. Speech Communication 27(34):281–298.
  </subsection>
  <subsection id="2583.9.21">
   Rosenthal, D., and Okuno, H. G., eds. 1998. Computational Auditory Scene Analysis. Mahwah, New Jersey: Lawrence Erlbaum Associates.
  </subsection>
  <subsection id="2583.9.22">
   Slaney, M.; Naar, D.; and Lyon, R. F. 1994. Auditory model inversion for sound separation. In Proceedings of 1994 International Conference on Acoustics, Speech, and Signal Processing, volume 2, 77–80.
  </subsection>
  <subsection id="2583.9.23">
   Takanishi, A.; Masukawa, S.; Mori, Y.; and Ogawa, T. 1995.
  </subsection>
  <subsection id="2583.9.24">
   Development of an anthropomorphic auditory robot that localizes a sound direction (in japanese). Bulletin of the Centre for Informatics 20:24–32.
  </subsection>
 </paragraph>
</paper>