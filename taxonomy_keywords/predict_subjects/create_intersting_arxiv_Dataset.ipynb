{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import snappy\n",
    "import fastparquet\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('distributed.worker')\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>inproc://192.168.1.21/837/1</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.1.21:8787/status' target='_blank'>http://192.168.1.21:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>6.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'inproc://192.168.1.21/837/1' processes=1 threads=4, memory=6.00 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=1, threads_per_worker=4, processes=False, memory_limit='6GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset with all analazyble papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 7.56 s, total: 20.7 s\n",
      "Wall time: 27.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>label_subsection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2535.1.1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>\\n   This paper addresses the problem of track...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2535.2.1</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n   The complexity and sophistication of the ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2535.2.2</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n   2000)). We want to monitor the state of t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2535.2.3</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n   In this paper, we propose a different app...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2535.2.4</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n   which are expressed as discrete failure m...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19460</th>\n",
       "      <td>101223.24.3</td>\n",
       "      <td>9 CONCLUSION</td>\n",
       "      <td>\\n   Thanks to the attention mechanism, the pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19461</th>\n",
       "      <td>101223.24.4</td>\n",
       "      <td>9 CONCLUSION</td>\n",
       "      <td>\\n   We believe that the attention-based model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19462</th>\n",
       "      <td>101223.25.1</td>\n",
       "      <td>ACKNOWLEDGMENTS</td>\n",
       "      <td>\\n   We would like to thank Guy Waldman for de...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19463</th>\n",
       "      <td>101223.25.2</td>\n",
       "      <td>ACKNOWLEDGMENTS</td>\n",
       "      <td>\\n   The research leading to these results has...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19464</th>\n",
       "      <td>101223.25.3</td>\n",
       "      <td>ACKNOWLEDGMENTS</td>\n",
       "      <td>\\n   code2vec: Learning Distributed Representa...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4040382 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_subsection   paragraph_name  \\\n",
       "0          2535.1.1         Abstract   \n",
       "1          2535.2.1     Introduction   \n",
       "2          2535.2.2     Introduction   \n",
       "3          2535.2.3     Introduction   \n",
       "4          2535.2.4     Introduction   \n",
       "...             ...              ...   \n",
       "19460   101223.24.3     9 CONCLUSION   \n",
       "19461   101223.24.4     9 CONCLUSION   \n",
       "19462   101223.25.1  ACKNOWLEDGMENTS   \n",
       "19463   101223.25.2  ACKNOWLEDGMENTS   \n",
       "19464   101223.25.3  ACKNOWLEDGMENTS   \n",
       "\n",
       "                                         text_subsection label_subsection  \n",
       "0      \\n   This paper addresses the problem of track...             None  \n",
       "1      \\n   The complexity and sophistication of the ...             None  \n",
       "2      \\n   2000)). We want to monitor the state of t...             None  \n",
       "3      \\n   In this paper, we propose a different app...             None  \n",
       "4      \\n   which are expressed as discrete failure m...             None  \n",
       "...                                                  ...              ...  \n",
       "19460  \\n   Thanks to the attention mechanism, the pr...             None  \n",
       "19461  \\n   We believe that the attention-based model...             None  \n",
       "19462  \\n   We would like to thank Guy Waldman for de...             None  \n",
       "19463  \\n   The research leading to these results has...             None  \n",
       "19464  \\n   code2vec: Learning Distributed Representa...             None  \n",
       "\n",
       "[4040382 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parquets_dir = \"../../data/papers-parquets\"\n",
    "ddf = dd.read_parquet(parquets_dir, index=False, engine='fastparquet')\n",
    "#ddf = ddf.drop(columns = ['__null_dask_index__'], axis=1)\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset with predicted analazyble papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.31 s, sys: 2.45 s, total: 7.76 s\n",
      "Wall time: 8.69 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>predict_id_by_lr</th>\n",
       "      <th>predict_id_by_svc</th>\n",
       "      <th>predict_id_by_mnb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2535.1.1</td>\n",
       "      <td>paper address problem track diagnos complex sy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2535.2.1</td>\n",
       "      <td>complex sophist current gener industri process...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2535.2.2</td>\n",
       "      <td>want monitor state system reliabl detect abnor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2535.2.3</td>\n",
       "      <td>paper propos differ approach problem model com...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2535.2.4</td>\n",
       "      <td>express discret failur mode produc discontinuo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18169</th>\n",
       "      <td>101223.24.3</td>\n",
       "      <td>thank attent mechan predict result interpret p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18170</th>\n",
       "      <td>101223.24.4</td>\n",
       "      <td>believ attentionbas model use structur represe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18171</th>\n",
       "      <td>101223.25.1</td>\n",
       "      <td>thank waldman develop codevec websit thank mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18172</th>\n",
       "      <td>101223.25.2</td>\n",
       "      <td>lead result receiv fund european union seventh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18173</th>\n",
       "      <td>101223.25.3</td>\n",
       "      <td>codevec learn distribut represent code</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3943815 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_subsection                                    text_subsection  \\\n",
       "0          2535.1.1  paper address problem track diagnos complex sy...   \n",
       "1          2535.2.1  complex sophist current gener industri process...   \n",
       "2          2535.2.2  want monitor state system reliabl detect abnor...   \n",
       "3          2535.2.3  paper propos differ approach problem model com...   \n",
       "4          2535.2.4  express discret failur mode produc discontinuo...   \n",
       "...             ...                                                ...   \n",
       "18169   101223.24.3  thank attent mechan predict result interpret p...   \n",
       "18170   101223.24.4  believ attentionbas model use structur represe...   \n",
       "18171   101223.25.1  thank waldman develop codevec websit thank mil...   \n",
       "18172   101223.25.2  lead result receiv fund european union seventh...   \n",
       "18173   101223.25.3             codevec learn distribut represent code   \n",
       "\n",
       "       predict_id_by_lr  predict_id_by_svc  predict_id_by_mnb  \n",
       "0                     0                  0                  0  \n",
       "1                     0                  0                  0  \n",
       "2                     0                  0                  0  \n",
       "3                     0                  0                  0  \n",
       "4                     0                  0                  0  \n",
       "...                 ...                ...                ...  \n",
       "18169                 0                  0                  0  \n",
       "18170                 0                  0                  0  \n",
       "18171                 0                  0                  0  \n",
       "18172                 0                  0                  0  \n",
       "18173                 0                  0                  0  \n",
       "\n",
       "[3943815 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predict_parquets_dir = \"../../data/papers-predicted-parquets\"\n",
    "ddf_predicted = dd.read_parquet(predict_parquets_dir, index=False, engine='fastparquet')\n",
    "ddf_predicted = ddf_predicted.drop(columns = ['__null_dask_index__'], axis=1)\n",
    "ddf_predicted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.inproc - WARNING - Closing dangling queue in <InProc  local=inproc://192.168.1.21/837/1 remote=inproc://192.168.1.21/837/10>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>label_subsection</th>\n",
       "      <th>predict_id_by_svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2535.2.6</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>\\n   There are several advantages to the use o...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2535.3.22</td>\n",
       "      <td>The framework</td>\n",
       "      <td>\\n   Figure 2 shows a DBN created by this proc...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2535.4.2</td>\n",
       "      <td>Inference</td>\n",
       "      <td>\\n   We therefore build our algorithm starting...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2536.5.2</td>\n",
       "      <td>Combining Action Theories</td>\n",
       "      <td>\\n   We use two new fluents together with inHa...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2536.6.2</td>\n",
       "      <td>Combining Different Agents</td>\n",
       "      <td>\\n   Money Send/Receive Agents Interactions Bu...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161713</th>\n",
       "      <td>101223.17.6</td>\n",
       "      <td>6 EVALUATION</td>\n",
       "      <td>\\n   Following recent work which found a large...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161714</th>\n",
       "      <td>101223.18.13</td>\n",
       "      <td>6.1 Quantitative Evaluation</td>\n",
       "      <td>\\n   With 20% of the amounts of data, the F1 s...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161715</th>\n",
       "      <td>101223.21.8</td>\n",
       "      <td>6.4 Qualitative Evaluation</td>\n",
       "      <td>\\n   A : B C : D open : connect close : discon...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161716</th>\n",
       "      <td>101223.23.12</td>\n",
       "      <td>8 RELATED WORK</td>\n",
       "      <td>\\n   In this work, we use distributed represen...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161717</th>\n",
       "      <td>101223.24.1</td>\n",
       "      <td>9 CONCLUSION</td>\n",
       "      <td>\\n   We presented a new attention-based neural...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4040382 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_subsection               paragraph_name  \\\n",
       "0           2535.2.6                 Introduction   \n",
       "1          2535.3.22                The framework   \n",
       "2           2535.4.2                    Inference   \n",
       "3           2536.5.2    Combining Action Theories   \n",
       "4           2536.6.2   Combining Different Agents   \n",
       "...              ...                          ...   \n",
       "161713   101223.17.6                 6 EVALUATION   \n",
       "161714  101223.18.13  6.1 Quantitative Evaluation   \n",
       "161715   101223.21.8   6.4 Qualitative Evaluation   \n",
       "161716  101223.23.12               8 RELATED WORK   \n",
       "161717   101223.24.1                 9 CONCLUSION   \n",
       "\n",
       "                                          text_subsection label_subsection  \\\n",
       "0       \\n   There are several advantages to the use o...             None   \n",
       "1       \\n   Figure 2 shows a DBN created by this proc...             None   \n",
       "2       \\n   We therefore build our algorithm starting...             None   \n",
       "3       \\n   We use two new fluents together with inHa...             None   \n",
       "4       \\n   Money Send/Receive Agents Interactions Bu...             None   \n",
       "...                                                   ...              ...   \n",
       "161713  \\n   Following recent work which found a large...             None   \n",
       "161714  \\n   With 20% of the amounts of data, the F1 s...             None   \n",
       "161715  \\n   A : B C : D open : connect close : discon...             None   \n",
       "161716  \\n   In this work, we use distributed represen...             None   \n",
       "161717  \\n   We presented a new attention-based neural...             None   \n",
       "\n",
       "        predict_id_by_svc  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "...                   ...  \n",
       "161713                0.0  \n",
       "161714                0.0  \n",
       "161715                0.0  \n",
       "161716                0.0  \n",
       "161717                0.0  \n",
       "\n",
       "[4040382 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_tmp = ddf_predicted.loc[:, ['id_subsection','predict_id_by_svc']]\n",
    "ddf_res = ddf.merge(ddf_tmp, how='left', on='id_subsection')\n",
    "ddf_res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    NaN\n",
       "Name: predict_id_by_svc, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del ddf\n",
    "del ddf_predicted\n",
    "del ddf_tmp\n",
    "ddf_res.predict_id_by_svc.unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96567\n",
      "96567\n"
     ]
    }
   ],
   "source": [
    "# check on 'predict_id_by_svc'\n",
    "print(len(ddf_res.loc[ddf_res.predict_id_by_svc.isna()].compute()))\n",
    "print(len(ddf_res.loc[ddf_res.predict_id_by_svc.isna() & (ddf_res.label_subsection.str != None)].compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find interesting subsections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_res['interesting_subsection'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsections in important paragraph (to understand the context)\n",
    "relevant_paragraphs = ['abstract',\n",
    "                       'introduction',\n",
    "                       'background',\n",
    "                       'preliminaries',\n",
    "                       'motiv',             # e.g. \"motivations\", \"motivating example\"\n",
    "                       'description',       # e.g. \"model description\"\n",
    "                       'overview',          # e.g. \"system overview\"\n",
    "                       'problem',           # e.g. \"problem definition\", \"the ... Problem\"\n",
    "                       'application',\n",
    "                       'scenario',\n",
    "                       'goal',              # e.g. \"design goals\"\n",
    "                       'discussion',\n",
    "                       'work',              # e.g. \"future work\", \"related work\"\n",
    "                       'result',\n",
    "                       'conclusion',\n",
    "                       #'experiment',\n",
    "                       #'architecture',\n",
    "                       'domain',            # e.g \"domain modelling\"\n",
    "                      ]\n",
    "\n",
    "def is_relevant_paragraph(paragraph_name):\n",
    "    for p in relevant_paragraphs:\n",
    "        if p in paragraph_name.lower():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "ddf_res['interesting_subsection'] = ddf_res['paragraph_name'].apply(lambda x: is_relevant_paragraph(x), meta=(None, 'int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502209\n"
     ]
    }
   ],
   "source": [
    "print(len(ddf_res.loc[ddf_res.interesting_subsection == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection defined or predicted as 'PD'\n",
    "condition = (ddf_res.label_subsection == 'PD') | (ddf_res.predict_id_by_svc == 1)\n",
    "ddf_res['interesting_subsection'] = ddf_res['interesting_subsection'].mask(condition, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538429\n"
     ]
    }
   ],
   "source": [
    "print(len(ddf_res.loc[ddf_res.interesting_subsection == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all arxiv paper ids and Keep only subsections of arxiv papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_arxiv_paper = '../../data/LIST_PAPERS_arxiv.txt'\n",
    "arxiv_ids = []\n",
    "with open(list_arxiv_paper, 'r') as f:\n",
    "    arxiv_ids = [line.split('\\t\\t')[0] for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>interesting_subsection</th>\n",
       "      <th>id_paper</th>\n",
       "      <th>id_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55410</th>\n",
       "      <td>41513.3.3</td>\n",
       "      <td>2 Data</td>\n",
       "      <td>\\n   2  Figure 2: These images depict the robo...</td>\n",
       "      <td>0</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55411</th>\n",
       "      <td>41513.4.2</td>\n",
       "      <td>3 Method</td>\n",
       "      <td>\\n   x̂t+1 = F(xt, ut; Wfwd) (1) ˆ ut = G(xt, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55412</th>\n",
       "      <td>41513.6.2</td>\n",
       "      <td>3.2 Evaluation Procedure</td>\n",
       "      <td>\\n   succeeds at achieving the goal configurat...</td>\n",
       "      <td>0</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55413</th>\n",
       "      <td>41513.8.1</td>\n",
       "      <td>4 Results</td>\n",
       "      <td>\\n   The robot was tasked to displace objects ...</td>\n",
       "      <td>1</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55414</th>\n",
       "      <td>41513.8.4</td>\n",
       "      <td>4 Results</td>\n",
       "      <td>\\n   poking object by small distances). Row 3 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161713</th>\n",
       "      <td>101223.17.6</td>\n",
       "      <td>6 EVALUATION</td>\n",
       "      <td>\\n   Following recent work which found a large...</td>\n",
       "      <td>0</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161714</th>\n",
       "      <td>101223.18.13</td>\n",
       "      <td>6.1 Quantitative Evaluation</td>\n",
       "      <td>\\n   With 20% of the amounts of data, the F1 s...</td>\n",
       "      <td>0</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161715</th>\n",
       "      <td>101223.21.8</td>\n",
       "      <td>6.4 Qualitative Evaluation</td>\n",
       "      <td>\\n   A : B C : D open : connect close : discon...</td>\n",
       "      <td>0</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161716</th>\n",
       "      <td>101223.23.12</td>\n",
       "      <td>8 RELATED WORK</td>\n",
       "      <td>\\n   In this work, we use distributed represen...</td>\n",
       "      <td>1</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161717</th>\n",
       "      <td>101223.24.1</td>\n",
       "      <td>9 CONCLUSION</td>\n",
       "      <td>\\n   We presented a new attention-based neural...</td>\n",
       "      <td>1</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1896197 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_subsection               paragraph_name  \\\n",
       "55410      41513.3.3                       2 Data   \n",
       "55411      41513.4.2                     3 Method   \n",
       "55412      41513.6.2     3.2 Evaluation Procedure   \n",
       "55413      41513.8.1                    4 Results   \n",
       "55414      41513.8.4                    4 Results   \n",
       "...              ...                          ...   \n",
       "161713   101223.17.6                 6 EVALUATION   \n",
       "161714  101223.18.13  6.1 Quantitative Evaluation   \n",
       "161715   101223.21.8   6.4 Qualitative Evaluation   \n",
       "161716  101223.23.12               8 RELATED WORK   \n",
       "161717   101223.24.1                 9 CONCLUSION   \n",
       "\n",
       "                                          text_subsection  \\\n",
       "55410   \\n   2  Figure 2: These images depict the robo...   \n",
       "55411   \\n   x̂t+1 = F(xt, ut; Wfwd) (1) ˆ ut = G(xt, ...   \n",
       "55412   \\n   succeeds at achieving the goal configurat...   \n",
       "55413   \\n   The robot was tasked to displace objects ...   \n",
       "55414   \\n   poking object by small distances). Row 3 ...   \n",
       "...                                                   ...   \n",
       "161713  \\n   Following recent work which found a large...   \n",
       "161714  \\n   With 20% of the amounts of data, the F1 s...   \n",
       "161715  \\n   A : B C : D open : connect close : discon...   \n",
       "161716  \\n   In this work, we use distributed represen...   \n",
       "161717  \\n   We presented a new attention-based neural...   \n",
       "\n",
       "        interesting_subsection id_paper id_paragraph  \n",
       "55410                        0    41513      41513.3  \n",
       "55411                        0    41513      41513.4  \n",
       "55412                        0    41513      41513.6  \n",
       "55413                        1    41513      41513.8  \n",
       "55414                        1    41513      41513.8  \n",
       "...                        ...      ...          ...  \n",
       "161713                       0   101223    101223.17  \n",
       "161714                       0   101223    101223.18  \n",
       "161715                       0   101223    101223.21  \n",
       "161716                       1   101223    101223.23  \n",
       "161717                       1   101223    101223.24  \n",
       "\n",
       "[1896197 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_arxiv = ddf_res.drop(columns = ['label_subsection','predict_id_by_svc'], axis=1)\n",
    "ddf_arxiv['id_paper'] = ddf_arxiv.id_subsection.apply(lambda x: x[:x.find('.')], meta=(None, 'object'))\n",
    "ddf_arxiv['id_paragraph'] = ddf_arxiv.id_subsection.apply(lambda x: x[:x.rfind('.')], meta=(None, 'object'))\n",
    "ddf_arxiv = ddf_arxiv.loc[ddf_arxiv.id_paper.isin(arxiv_ids)]\n",
    "ddf_arxiv.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only paragraph with at a least a subsection =1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114064"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_least_series = ddf_arxiv.groupby(['id_paragraph']).interesting_subsection.sum().compute()\n",
    "at_least_paragraph_ids = list(at_least_series[at_least_series > 0].index)\n",
    "len(at_least_paragraph_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>interesting_subsection</th>\n",
       "      <th>id_paper</th>\n",
       "      <th>id_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55413</th>\n",
       "      <td>41513.8.1</td>\n",
       "      <td>4 Results</td>\n",
       "      <td>\\n   The robot was tasked to displace objects ...</td>\n",
       "      <td>1</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55414</th>\n",
       "      <td>41513.8.4</td>\n",
       "      <td>4 Results</td>\n",
       "      <td>\\n   poking object by small distances). Row 3 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55415</th>\n",
       "      <td>41513.11.1</td>\n",
       "      <td>6 Discussion and Future Work</td>\n",
       "      <td>\\n   In this work we propose to learn “intuiti...</td>\n",
       "      <td>1</td>\n",
       "      <td>41513</td>\n",
       "      <td>41513.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82188</th>\n",
       "      <td>61823.1.1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>\\n   Nonnegative matrix factorization (NMF) ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>61823</td>\n",
       "      <td>61823.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82189</th>\n",
       "      <td>61824.2.5</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>\\n   This paper provides a mathematical analys...</td>\n",
       "      <td>1</td>\n",
       "      <td>61824</td>\n",
       "      <td>61824.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161708</th>\n",
       "      <td>101222.4.3</td>\n",
       "      <td>3 LEARNING DECISION GRAPHS</td>\n",
       "      <td>\\n   As we saw in the previous section, the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>101222</td>\n",
       "      <td>101222.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161709</th>\n",
       "      <td>101222.4.14</td>\n",
       "      <td>3 LEARNING DECISION GRAPHS</td>\n",
       "      <td>\\n   That is, Θ−1 (θ) = {i, j, k|Θ(i, j, k) = ...</td>\n",
       "      <td>0</td>\n",
       "      <td>101222</td>\n",
       "      <td>101222.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161710</th>\n",
       "      <td>101223.8.17</td>\n",
       "      <td>2.1 Motivating Example</td>\n",
       "      <td>\\n   • Although our model is based on a neural...</td>\n",
       "      <td>1</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161716</th>\n",
       "      <td>101223.23.12</td>\n",
       "      <td>8 RELATED WORK</td>\n",
       "      <td>\\n   In this work, we use distributed represen...</td>\n",
       "      <td>1</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161717</th>\n",
       "      <td>101223.24.1</td>\n",
       "      <td>9 CONCLUSION</td>\n",
       "      <td>\\n   We presented a new attention-based neural...</td>\n",
       "      <td>1</td>\n",
       "      <td>101223</td>\n",
       "      <td>101223.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880821 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_subsection                paragraph_name  \\\n",
       "55413      41513.8.1                     4 Results   \n",
       "55414      41513.8.4                     4 Results   \n",
       "55415     41513.11.1  6 Discussion and Future Work   \n",
       "82188      61823.1.1                      Abstract   \n",
       "82189      61824.2.5                1 Introduction   \n",
       "...              ...                           ...   \n",
       "161708    101222.4.3    3 LEARNING DECISION GRAPHS   \n",
       "161709   101222.4.14    3 LEARNING DECISION GRAPHS   \n",
       "161710   101223.8.17        2.1 Motivating Example   \n",
       "161716  101223.23.12                8 RELATED WORK   \n",
       "161717   101223.24.1                  9 CONCLUSION   \n",
       "\n",
       "                                          text_subsection  \\\n",
       "55413   \\n   The robot was tasked to displace objects ...   \n",
       "55414   \\n   poking object by small distances). Row 3 ...   \n",
       "55415   \\n   In this work we propose to learn “intuiti...   \n",
       "82188   \\n   Nonnegative matrix factorization (NMF) ha...   \n",
       "82189   \\n   This paper provides a mathematical analys...   \n",
       "...                                                   ...   \n",
       "161708  \\n   As we saw in the previous section, the st...   \n",
       "161709  \\n   That is, Θ−1 (θ) = {i, j, k|Θ(i, j, k) = ...   \n",
       "161710  \\n   • Although our model is based on a neural...   \n",
       "161716  \\n   In this work, we use distributed represen...   \n",
       "161717  \\n   We presented a new attention-based neural...   \n",
       "\n",
       "        interesting_subsection id_paper id_paragraph  \n",
       "55413                        1    41513      41513.8  \n",
       "55414                        1    41513      41513.8  \n",
       "55415                        1    41513     41513.11  \n",
       "82188                        1    61823      61823.1  \n",
       "82189                        1    61824      61824.2  \n",
       "...                        ...      ...          ...  \n",
       "161708                       0   101222     101222.4  \n",
       "161709                       0   101222     101222.4  \n",
       "161710                       1   101223     101223.8  \n",
       "161716                       1   101223    101223.23  \n",
       "161717                       1   101223    101223.24  \n",
       "\n",
       "[880821 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_arxiv = ddf_arxiv.loc[ddf_arxiv.id_paragraph.isin(at_least_paragraph_ids)]\n",
    "ddf_arxiv.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by 'id_subsection' value to build the text for the paper (with only interesting paragraphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>id_paper</th>\n",
       "      <th>id_paragraph</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_subsection</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100002.001.0001</th>\n",
       "      <td>Abstract</td>\n",
       "      <td>\\n   We consider the problem of learning the f...</td>\n",
       "      <td>100002</td>\n",
       "      <td>100002.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002.001.0002</th>\n",
       "      <td>Abstract</td>\n",
       "      <td>\\n   This is in some sense the second step aft...</td>\n",
       "      <td>100002</td>\n",
       "      <td>100002.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002.002.0001</th>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>\\n   Large parts of the literature on causalit...</td>\n",
       "      <td>100002</td>\n",
       "      <td>100002.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002.002.0002</th>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>\\n   The starting point of this paper is to co...</td>\n",
       "      <td>100002</td>\n",
       "      <td>100002.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002.002.0003</th>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>\\n   That is, it is known which variables are ...</td>\n",
       "      <td>100002</td>\n",
       "      <td>100002.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998.010.0004</th>\n",
       "      <td>5 APPENDIX: BRIEF BACKGROUND</td>\n",
       "      <td>\\n   (2015); Krizhevsky et al. (2009); Deng et...</td>\n",
       "      <td>99998</td>\n",
       "      <td>99998.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998.010.0005</th>\n",
       "      <td>5 APPENDIX: BRIEF BACKGROUND</td>\n",
       "      <td>\\n   The convolution layer can be defined as  ...</td>\n",
       "      <td>99998</td>\n",
       "      <td>99998.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998.010.0006</th>\n",
       "      <td>5 APPENDIX: BRIEF BACKGROUND</td>\n",
       "      <td>\\n   The fully connected layer is defined as  ...</td>\n",
       "      <td>99998</td>\n",
       "      <td>99998.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998.010.0007</th>\n",
       "      <td>5 APPENDIX: BRIEF BACKGROUND</td>\n",
       "      <td>\\n   The typical batch normalization layer can...</td>\n",
       "      <td>99998</td>\n",
       "      <td>99998.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998.010.0008</th>\n",
       "      <td>5 APPENDIX: BRIEF BACKGROUND</td>\n",
       "      <td>\\n   2 In this context it is also common to us...</td>\n",
       "      <td>99998</td>\n",
       "      <td>99998.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880821 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               paragraph_name  \\\n",
       "id_subsection                                   \n",
       "100002.001.0001                      Abstract   \n",
       "100002.001.0002                      Abstract   \n",
       "100002.002.0001                1 Introduction   \n",
       "100002.002.0002                1 Introduction   \n",
       "100002.002.0003                1 Introduction   \n",
       "...                                       ...   \n",
       "99998.010.0004   5 APPENDIX: BRIEF BACKGROUND   \n",
       "99998.010.0005   5 APPENDIX: BRIEF BACKGROUND   \n",
       "99998.010.0006   5 APPENDIX: BRIEF BACKGROUND   \n",
       "99998.010.0007   5 APPENDIX: BRIEF BACKGROUND   \n",
       "99998.010.0008   5 APPENDIX: BRIEF BACKGROUND   \n",
       "\n",
       "                                                   text_subsection id_paper  \\\n",
       "id_subsection                                                                 \n",
       "100002.001.0001  \\n   We consider the problem of learning the f...   100002   \n",
       "100002.001.0002  \\n   This is in some sense the second step aft...   100002   \n",
       "100002.002.0001  \\n   Large parts of the literature on causalit...   100002   \n",
       "100002.002.0002  \\n   The starting point of this paper is to co...   100002   \n",
       "100002.002.0003  \\n   That is, it is known which variables are ...   100002   \n",
       "...                                                            ...      ...   \n",
       "99998.010.0004   \\n   (2015); Krizhevsky et al. (2009); Deng et...    99998   \n",
       "99998.010.0005   \\n   The convolution layer can be defined as  ...    99998   \n",
       "99998.010.0006   \\n   The fully connected layer is defined as  ...    99998   \n",
       "99998.010.0007   \\n   The typical batch normalization layer can...    99998   \n",
       "99998.010.0008   \\n   2 In this context it is also common to us...    99998   \n",
       "\n",
       "                id_paragraph  \n",
       "id_subsection                 \n",
       "100002.001.0001     100002.1  \n",
       "100002.001.0002     100002.1  \n",
       "100002.002.0001     100002.2  \n",
       "100002.002.0002     100002.2  \n",
       "100002.002.0003     100002.2  \n",
       "...                      ...  \n",
       "99998.010.0004      99998.10  \n",
       "99998.010.0005      99998.10  \n",
       "99998.010.0006      99998.10  \n",
       "99998.010.0007      99998.10  \n",
       "99998.010.0008      99998.10  \n",
       "\n",
       "[880821 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjust_id_subsection(id_subsection): #6 #3 #4\n",
    "    ids = id_subsection.split('.')\n",
    "    return \"{0}.{1:03}.{2:04}\".format(int(ids[0]), int(ids[1]), int(ids[2]))\n",
    "\n",
    "ddf_text = ddf_arxiv.drop('interesting_subsection', axis=1)\n",
    "ddf_text['id_subsection'] = ddf_text.id_subsection.apply(lambda x: adjust_id_subsection(x), meta=(None, 'object'))\n",
    "ddf_text = ddf_text.set_index('id_subsection')\n",
    "ddf_text = ddf_text.map_partitions(lambda x: x.sort_index())\n",
    "ddf_text.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>id_paper</th>\n",
       "      <th>id_paragraph</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_subsection</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64514.006.0032</th>\n",
       "      <td>5. Transformations of Deep Residual Networks</td>\n",
       "      <td>None</td>\n",
       "      <td>64514</td>\n",
       "      <td>64514.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64754.009.0020</th>\n",
       "      <td>3.3 The hardness of strong-minimality</td>\n",
       "      <td>None</td>\n",
       "      <td>64754</td>\n",
       "      <td>64754.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66189.005.0008</th>\n",
       "      <td>2.2. Neural networks 2.2.1. Single-layer neura...</td>\n",
       "      <td>None</td>\n",
       "      <td>66189</td>\n",
       "      <td>66189.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67100.008.0028</th>\n",
       "      <td>5.2 Proof of Theorem 6</td>\n",
       "      <td>None</td>\n",
       "      <td>67100</td>\n",
       "      <td>67100.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67100.009.0028</th>\n",
       "      <td>5.3 A Finite-Sample Generalization Result and ...</td>\n",
       "      <td>None</td>\n",
       "      <td>67100</td>\n",
       "      <td>67100.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67409.002.0003</th>\n",
       "      <td>1. Introduction</td>\n",
       "      <td>None</td>\n",
       "      <td>67409</td>\n",
       "      <td>67409.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67799.007.0013</th>\n",
       "      <td>A Stability results on time-varying linear sys...</td>\n",
       "      <td>None</td>\n",
       "      <td>67799</td>\n",
       "      <td>67799.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68118.012.0003</th>\n",
       "      <td>Results and Analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>68118</td>\n",
       "      <td>68118.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69290.022.0007</th>\n",
       "      <td>A.3. Negative Result: Non Inverse-Degree Dynam...</td>\n",
       "      <td>None</td>\n",
       "      <td>69290</td>\n",
       "      <td>69290.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70793.005.0014</th>\n",
       "      <td>4. Experimental Results</td>\n",
       "      <td>None</td>\n",
       "      <td>70793</td>\n",
       "      <td>70793.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71419.004.0027</th>\n",
       "      <td>2.1 wFM on M as a generalization of convolution</td>\n",
       "      <td>None</td>\n",
       "      <td>71419</td>\n",
       "      <td>71419.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71428.011.0011</th>\n",
       "      <td>5.1. Almost-sure Upper Bound</td>\n",
       "      <td>None</td>\n",
       "      <td>71428</td>\n",
       "      <td>71428.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72288.012.0020</th>\n",
       "      <td>5.1 Proof of Results for Homogeneous Diffusion</td>\n",
       "      <td>None</td>\n",
       "      <td>72288</td>\n",
       "      <td>72288.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75808.004.0054</th>\n",
       "      <td>3. The stochastic case</td>\n",
       "      <td>None</td>\n",
       "      <td>75808</td>\n",
       "      <td>75808.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79818.006.0032</th>\n",
       "      <td>5. Experiments and Results</td>\n",
       "      <td>None</td>\n",
       "      <td>79818</td>\n",
       "      <td>79818.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82027.006.0007</th>\n",
       "      <td>4 A Definition of Joint Entropy using Hadamard...</td>\n",
       "      <td>None</td>\n",
       "      <td>82027</td>\n",
       "      <td>82027.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82027.012.0017</th>\n",
       "      <td>6.2 Estimating the Spectrum of G</td>\n",
       "      <td>None</td>\n",
       "      <td>82027</td>\n",
       "      <td>82027.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85248.011.0014</th>\n",
       "      <td>7 Discussion</td>\n",
       "      <td>None</td>\n",
       "      <td>85248</td>\n",
       "      <td>85248.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86274.011.0033</th>\n",
       "      <td>5. A NEW POR ALGORITHM FRAMEWORK FOR PLANNING</td>\n",
       "      <td>None</td>\n",
       "      <td>86274</td>\n",
       "      <td>86274.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87324.030.0055</th>\n",
       "      <td>A.3. Proof of Lemma A.1. We prove Lemma A.</td>\n",
       "      <td>None</td>\n",
       "      <td>87324</td>\n",
       "      <td>87324.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88996.020.0007</th>\n",
       "      <td>Appendix B. Perturbation Result</td>\n",
       "      <td>None</td>\n",
       "      <td>88996</td>\n",
       "      <td>88996.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320.010.0016</th>\n",
       "      <td>5.1. Preliminaries.</td>\n",
       "      <td>None</td>\n",
       "      <td>89320</td>\n",
       "      <td>89320.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90146.027.0016</th>\n",
       "      <td>B.4. Supporting proofs.</td>\n",
       "      <td>None</td>\n",
       "      <td>90146</td>\n",
       "      <td>90146.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91279.005.0043</th>\n",
       "      <td>4 Applications to Identity Testing</td>\n",
       "      <td>None</td>\n",
       "      <td>91279</td>\n",
       "      <td>91279.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98169.005.0018</th>\n",
       "      <td>2. Geometry of the loss landscape L - general ...</td>\n",
       "      <td>None</td>\n",
       "      <td>98169</td>\n",
       "      <td>98169.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98825.016.0004</th>\n",
       "      <td>Searching for an Optimal Network Design</td>\n",
       "      <td>None</td>\n",
       "      <td>98825</td>\n",
       "      <td>98825.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99126.006.0004</th>\n",
       "      <td>5 Results</td>\n",
       "      <td>None</td>\n",
       "      <td>99126</td>\n",
       "      <td>99126.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   paragraph_name  \\\n",
       "id_subsection                                                       \n",
       "64514.006.0032       5. Transformations of Deep Residual Networks   \n",
       "64754.009.0020              3.3 The hardness of strong-minimality   \n",
       "66189.005.0008  2.2. Neural networks 2.2.1. Single-layer neura...   \n",
       "67100.008.0028                             5.2 Proof of Theorem 6   \n",
       "67100.009.0028  5.3 A Finite-Sample Generalization Result and ...   \n",
       "67409.002.0003                                    1. Introduction   \n",
       "67799.007.0013  A Stability results on time-varying linear sys...   \n",
       "68118.012.0003                               Results and Analysis   \n",
       "69290.022.0007  A.3. Negative Result: Non Inverse-Degree Dynam...   \n",
       "70793.005.0014                            4. Experimental Results   \n",
       "71419.004.0027    2.1 wFM on M as a generalization of convolution   \n",
       "71428.011.0011                       5.1. Almost-sure Upper Bound   \n",
       "72288.012.0020     5.1 Proof of Results for Homogeneous Diffusion   \n",
       "75808.004.0054                             3. The stochastic case   \n",
       "79818.006.0032                         5. Experiments and Results   \n",
       "82027.006.0007  4 A Definition of Joint Entropy using Hadamard...   \n",
       "82027.012.0017                   6.2 Estimating the Spectrum of G   \n",
       "85248.011.0014                                       7 Discussion   \n",
       "86274.011.0033      5. A NEW POR ALGORITHM FRAMEWORK FOR PLANNING   \n",
       "87324.030.0055         A.3. Proof of Lemma A.1. We prove Lemma A.   \n",
       "88996.020.0007                    Appendix B. Perturbation Result   \n",
       "89320.010.0016                                5.1. Preliminaries.   \n",
       "90146.027.0016                            B.4. Supporting proofs.   \n",
       "91279.005.0043                 4 Applications to Identity Testing   \n",
       "98169.005.0018  2. Geometry of the loss landscape L - general ...   \n",
       "98825.016.0004            Searching for an Optimal Network Design   \n",
       "99126.006.0004                                          5 Results   \n",
       "\n",
       "               text_subsection id_paper id_paragraph  \n",
       "id_subsection                                         \n",
       "64514.006.0032            None    64514      64514.6  \n",
       "64754.009.0020            None    64754      64754.9  \n",
       "66189.005.0008            None    66189      66189.5  \n",
       "67100.008.0028            None    67100      67100.8  \n",
       "67100.009.0028            None    67100      67100.9  \n",
       "67409.002.0003            None    67409      67409.2  \n",
       "67799.007.0013            None    67799      67799.7  \n",
       "68118.012.0003            None    68118     68118.12  \n",
       "69290.022.0007            None    69290     69290.22  \n",
       "70793.005.0014            None    70793      70793.5  \n",
       "71419.004.0027            None    71419      71419.4  \n",
       "71428.011.0011            None    71428     71428.11  \n",
       "72288.012.0020            None    72288     72288.12  \n",
       "75808.004.0054            None    75808      75808.4  \n",
       "79818.006.0032            None    79818      79818.6  \n",
       "82027.006.0007            None    82027      82027.6  \n",
       "82027.012.0017            None    82027     82027.12  \n",
       "85248.011.0014            None    85248     85248.11  \n",
       "86274.011.0033            None    86274     86274.11  \n",
       "87324.030.0055            None    87324     87324.30  \n",
       "88996.020.0007            None    88996     88996.20  \n",
       "89320.010.0016            None    89320     89320.10  \n",
       "90146.027.0016            None    90146     90146.27  \n",
       "91279.005.0043            None    91279      91279.5  \n",
       "98169.005.0018            None    98169      98169.5  \n",
       "98825.016.0004            None    98825     98825.16  \n",
       "99126.006.0004            None    99126      99126.6  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_text.loc[ddf_text.text_subsection.isna()].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>id_paper</th>\n",
       "      <th>id_paragraph</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_subsection</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [paragraph_name, text_subsection, id_paper, id_paragraph]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_text = ddf_text.dropna(subset=['text_subsection'])\n",
    "ddf_text.loc[ddf_text.text_subsection.isna()].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_paper\n",
       "100036    \\n   This paper presents a multimodal biometri...\n",
       "100075    \\n   Randomized trials, also known as A/B test...\n",
       "100147    \\n   The deep Q-network (DQN) and return-based...\n",
       "100161    \\n   We study the problem of learning a good s...\n",
       "100208    \\n   We present a novel method of compression ...\n",
       "                                ...                        \n",
       "99842     \\n   for the case of combining classifiers. We...\n",
       "99920     \\n   We propose a novel dialogue modeling fram...\n",
       "99922     \\n   We study a classification problem where e...\n",
       "99932     \\n   Many challenging image processing tasks c...\n",
       "99988     \\n   Modeling physics system, learning molecul...\n",
       "Length: 21802, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use '|' to check correctness of reconstrucion:\n",
    "tmp = ddf_text.groupby('id_paper')['text_subsection'].apply('|'.join, meta=(None, 'object'))\n",
    "tmp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.Series"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_paper</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100036</th>\n",
       "      <td>\\n   This paper presents a multimodal biometri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100075</th>\n",
       "      <td>\\n   Randomized trials, also known as A/B test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100147</th>\n",
       "      <td>\\n   The deep Q-network (DQN) and return-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100161</th>\n",
       "      <td>\\n   We study the problem of learning a good s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100208</th>\n",
       "      <td>\\n   We present a novel method of compression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99842</th>\n",
       "      <td>\\n   for the case of combining classifiers. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99920</th>\n",
       "      <td>\\n   We propose a novel dialogue modeling fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99922</th>\n",
       "      <td>\\n   We study a classification problem where e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99932</th>\n",
       "      <td>\\n   Many challenging image processing tasks c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>\\n   Modeling physics system, learning molecul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21802 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0\n",
       "id_paper                                                   \n",
       "100036    \\n   This paper presents a multimodal biometri...\n",
       "100075    \\n   Randomized trials, also known as A/B test...\n",
       "100147    \\n   The deep Q-network (DQN) and return-based...\n",
       "100161    \\n   We study the problem of learning a good s...\n",
       "100208    \\n   We present a novel method of compression ...\n",
       "...                                                     ...\n",
       "99842     \\n   for the case of combining classifiers. We...\n",
       "99920     \\n   We propose a novel dialogue modeling fram...\n",
       "99922     \\n   We study a classification problem where e...\n",
       "99932     \\n   Many challenging image processing tasks c...\n",
       "99988     \\n   Modeling physics system, learning molecul...\n",
       "\n",
       "[21802 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = tmp.to_frame().compute()\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100036</td>\n",
       "      <td>\\n   This paper presents a multimodal biometri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100075</td>\n",
       "      <td>\\n   Randomized trials, also known as A/B test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100147</td>\n",
       "      <td>\\n   The deep Q-network (DQN) and return-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100161</td>\n",
       "      <td>\\n   We study the problem of learning a good s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100208</td>\n",
       "      <td>\\n   We present a novel method of compression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21797</th>\n",
       "      <td>99842</td>\n",
       "      <td>\\n   for the case of combining classifiers. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21798</th>\n",
       "      <td>99920</td>\n",
       "      <td>\\n   We propose a novel dialogue modeling fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21799</th>\n",
       "      <td>99922</td>\n",
       "      <td>\\n   We study a classification problem where e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21800</th>\n",
       "      <td>99932</td>\n",
       "      <td>\\n   Many challenging image processing tasks c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21801</th>\n",
       "      <td>99988</td>\n",
       "      <td>\\n   Modeling physics system, learning molecul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21802 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_id                                               text\n",
       "0       100036  \\n   This paper presents a multimodal biometri...\n",
       "1       100075  \\n   Randomized trials, also known as A/B test...\n",
       "2       100147  \\n   The deep Q-network (DQN) and return-based...\n",
       "3       100161  \\n   We study the problem of learning a good s...\n",
       "4       100208  \\n   We present a novel method of compression ...\n",
       "...        ...                                                ...\n",
       "21797    99842  \\n   for the case of combining classifiers. We...\n",
       "21798    99920  \\n   We propose a novel dialogue modeling fram...\n",
       "21799    99922  \\n   We study a classification problem where e...\n",
       "21800    99932  \\n   Many challenging image processing tasks c...\n",
       "21801    99988  \\n   Modeling physics system, learning molecul...\n",
       "\n",
       "[21802 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = df_text.reset_index()\n",
    "df_text.rename(columns={'id_paper': 'paper_id', 0: 'text'}, inplace=True)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   We consider the problem of learning the functions computing children from parents in a Structural Causal Model once the underlying causal graph has been identified.\\n  |\\n   This is in some sense the second step after causal discovery. Taking a probabilistic approach to estimating these functions, we derive a natural myopic active learning scheme that identifies the intervention which is optimally informative about all of the unknown functions jointly, given previously observed data. We test the derived algorithms on simple examples, to demonstrate that they produce a structured exploration policy that significantly improves on unstructured base-lines.\\n  |\\n   Large parts of the literature on causality are concerned with learning the causal graph of a system of random variables [Spirtes et al., 2000, Tong and Koller, 2001, Eberhardt, 2010, Hyttinen et al., 2013, Mooij et al., 2016]. Also known as causal discovery or causal inference, this problem is motivated by realistic problems in science: a biologist may wish to discover genes responsible for regulating other genes in a cell; a public health researcher may wish to know whether certain habits in a population (e. g. smoking) influence certain health outcomes (e. g. probability of developing lung cancer).\\n  |\\n   The starting point of this paper is to consider what should be done after the causal graph of a system of variables has already been identified (be it by causal discovery methods, or from prior knowledge).\\n  |\\n   That is, it is known which variables are functions of which other variables, but the precise functional relationships are still unknown. Thus, although we understand the causal relationships in a coarse sense, we will not be able to accurately predict the result of an intervention to the system, possibly with implications for decision making.\\n  |\\n   For instance, suppose that in a cell, Gene A up-regulates Gene B and Gene B down-regulates Gene C.\\n  |\\n   We know that reducing expression of Gene A will lead to a decrease in expression of Gene B which, in turn, will lead to an increase in expression of Gene C. However, without more precise knowledge of the relationships between genes, we will be unable to quantitatively predict the effect of applying a drug that reduces expression of Gene A by 20%. Similarly, if our goal is to reduce overall levels of lung cancer in the population, then knowing only that smoking causes cancer is insufficient to know what the best public health policy should be: would it be better if we could persuade 50% of smokers to stop smoking completely, or persuade every smoker to reduce their consumption by 50%? For a passive agent supplied with data generated by the system, learning the functional relationships between parents and children reduces simply to separate regression problems—one for each unknown function—once the causal graph is known. Many knowledge acquisition problems, however, can be phrased as a sequential decision making process in which the data that is received at the next point in time is affected by a decision made based on the data that has already been observed. A biologist does not blindly perform a series of costly and time-consuming experiments, only looking at the generated data once the last is over; the data from each experiment would be analysed before the next is performed, thus informing which experiment would be best to perform next.\\n  |\\n   arXiv:1706.10234v1 [stat.ML] 30 Jun 2017  X1 X2 X3 f2 f3 (a) The observational setting X1 X2 X3 f2 f3 (b) do(X1 = x1) X1 X2 X3 f3 (c) do(X2 = x2) Figure 1: Even in the simple setting of three variables whose graph is a chain, there is a non-trivial trade-off between the information one expects to gain by performing different interventions (see text).\\n  |\\n   Below, we formalise this problem using the language of Structural Causal Models (SCMs), also known as Structural Equation Models [Pearl, 2009, Bollen, 2014]. An SCM, in essence, consists of functions connecting child variables with their causal parents, and is equipped with a notion of intervention in which a variable (or subset of variables) is externally forced to take a particular value (or values). We use these interventions as an idealised mathematical representation of performing an experiment. We take a Bayesian approach to estimating the functional relationships between parents and children, which naturally gives rise to an active learning algorithm to decide on the next ‘experiment’ to perform.\\n  |\\n   While our approach works with causal graphs that are arbitrary DAGs, the non-triviality of this problem is apparent even in the simple case of three variables whose causal graph is a chain (Figure 1).\\n  |\\n   Our goal in this situation is, in a sense to be made precise later, to learn the functions f2 and f3. At each point in time, we must decide whether to perform one of the possible interventions or to passively observe the system, with each different action having some cost. If we make a passive observation, we will learn something about both f2 and f3, though only in areas where the distributions over X1 and X2 put probability mass. In the small sample setting, we are very unlikely to learn anything about the functions in areas of low probability of their inputs. If we intervene on X1 and choose what value to set it to, we can decide precisely where we want to learn about f2 and we will also learn about f3 in some region, although we would be uncertain about where. If we intervene on X2, we can learn a precise aspect of f3, but will learn nothing about f2. How should we decide which action to pick, given what we already know about f2 and f3? The problem considered in this paper and our approach to solving it have a close connection to ideas in Bayesian optimization [Jones et al., 1998, Osborne et al., 2009, Shahriari et al., 2016]. In Bayesian optimization, the goal is to find the extremum of a function that is expensive to evaluate, possibly exploiting known structure to speed up the search. Due to this expense, the information obtained from each evaluation should be used efficiently. In contrast, however, in our setting we are not simply interested in finding the extremum of an unknown function, but rather to learn the entire function (or set of functions) in some sense. Tong and Koller [2000] considered a similar setting, but only treated discrete variables.\\n  |\\n   Below, we begin with a formal definition of Structural Causal Models (Section 2). Section 3 states the precise problem we are tackling. Section 4 provides a Bayesian formulation for inference in this setting, which is then complemented by an active learning scheme to guide exploration (Section 5).\\n  |\\n   Section 6 provides empirical evaluations on synthetic toy examples.\\n  |\\n   Suppose there exists an SCM M = (S, I, PE) with S = {Xn = fn(Xpa(n)) + En : n = 1, . . . , N} and PE ∼ N(0, Λ) where Λ is diagonal. We assume that the graphical structure is known, but the functions fn themselves are not. For simplicity, we assume that Λ is known.3 We are given data D drawn from the observational and a variety of interventional distributions of M; each element of D is a tuple (i, x), where x is an independent draw from P do(i) X . We are interested in two separate tasks.\\n  |\\n   Problem 1: Estimating M. Using D, learn functions ˆ fn such that the estimated model c M = (Ŝ, I, PE) with Ŝ = {Xn = ˆ fn(Xpa(n)) + En : n = 1, . . . , N} is ‘close’ to the true model M in some sense. Part of this problem is to define a sensible notion of closeness between SCMs.\\n  |\\n   Problem 2: Active learning. We can select an intervention i ∈ I at some cost c(i) and observe a single draw from P do(i) X . Which i should be selected to ensure the next estimation c M after incorporating the new datum is as close to M as possible? The rest of this section is devoted to defining a risk functional to provide a notion of closeness between c M and M. There may be no single best way to define this notion of closeness, as desirable properties may be dependent on particular use case.4 Consider, for instance, the following scenarios in which the ultimate goal is to: • Approximate each function fn so that a practitioner can visually interpret the relationship between parents and children (e.g., identifying genes as ‘excitatory’ or ‘inhibitory’, or identifying a threshold dosage at which a drug under medical trial is considered toxic.).\\n  |\\n   • Predict the result of an intervention that cannot be practically carried out, or is potentially dangerous to do so (e.g., raising interest rates, or giving a patient a drug).\\n  |\\n   1 That is, the directed graph with nodes {1, . . . , N} with edges n → m if and only if n ∈ pa(m) is a DAG.\\n  |\\n   2 That is, for any e ∈ E, there is a unique x ∈ X such that xn = fn(xpa(n), en) for each n. This can be seen to be true by explicitly writing each xn as a function of the en by substituting the equations into one another.\\n  |\\n   3 This assumption could be relaxed to a more Bayesian approach involving a prior over the covariance matrix, which would reduce to running the algorithm derived in the following, but averaging its results over the posterior.\\n  |\\n   4 The same is true in the case of causal graph learning. See e. g. de Jongh and Druzdzel [2009].\\n  |\\n   3  • Better control a system in a variety of environments or conditions (e.g., learning the dynamics of a complex vehicle to be employed in variable conditions) We will suppose that for each function fn with domain Xpa(n) = Q m∈pa(n) Xm we are supplied with a probability measure Πn over Xpa(n) specifying the importance of learning the pointwise value of fn at each input xpa(n) ∈ Xpa(n). That is, Πn puts large amounts of mass in areas that we should learn fn precisely, small amounts of mass in areas that we should learn fn only approximately, and zero mass in areas for which we do not care about learning fn at all. For the (estimated) function ˆ fn, we use the risk functional Ln below. The weighted sum of these according to the importance of each function gives rise to the total risk L, where f = (fn)n=1,...,N and ˆ f = ( ˆ fn)n=1,...,N are vectors of the functions fn and ˆ fn, respectively.\\n  |\\n   Ln( ˆ fn||fn) = Z Xpa(n)  fn(x) − ˆ fn(x) 2 dΠn(x), L( ˆ f||f) = N X n=1 αnLn( ˆ fn||fn), αn ≥ 0.\\n  |\\n   We will assume for simplicity that αn = 1 for each n. Ln is also known as the Mean Integrated Squared Error [Tsybakov, 2009] and in the case that Πn = P do(∅) Xpa(n) , this coincides with a typical objective that would be minimised in a classical non-parametric statistical learning setting [Györfi et al., 2006]. It is worth considering other possible risk functionals that could be used, since the presence of the measures Πn is arguably somewhat arbitrary in the L that we consider. When learning the parameters of a statistical model, a commonly used objective with many separate justifications is to minimise the KL divergence KL[PX||P̂X] between the true data distribution PX and that implied by the learned model, P̂X. SCMs do not imply a single distribution over the variables X, but rather a family of distributions, one for each intervention: {P do(i) X : i ∈ I}. One may therefore wish to consider a separate loss for each interventional distribution and, for example, uniformly bound these losses over a subset of interventions I0 ⊆ I of interest.\\n  |\\n   Li KL( ˆ f||f) = KL h P do(i) X ||P̂ do(i) X i , LI0 KL( ˆ f||f) = sup i∈I0 Li KL( ˆ f||f).\\n  |\\n   Alternatively, one could replace the KL divergence with a different divergence measure or metric on distributions. For instance, one could use the Maximum Mean Discrepancy (MMD) corresponding to a characteristic kernel l [Sriperumbudur et al., 2008] Li MMDl ( ˆ f||f) = MMDl h P do(i) X ||P̂ do(i) X i , LI0 MMDl ( ˆ f||f) = sup i∈I0 Li MMDl ( ˆ f||f).\\n  |\\n   Though we do not analyse or derive active learning schemes for these risk functionals, they will be used in Section 6 to evaluate our algorithm. We leave their consideration for future work.\\n  |\\n   By taking a Bayesian approach to learning the vector of unknown functions f, Problem 1 can be reduced to a series of independent regression problems between input and output domains Xpa(n) and Xn for each n. A common choice of prior when learning functions is a Gaussian Process (GP) [Rasmussen and Williams, 2005]. For each function fn, we will assume a zero mean GP prior with kernel kn over the domain Xpa(n) 5 fn ∼ GP(0, kn).\\n  |\\n   Recall that we are given a dataset D consisting of elements (i, x) where x ∼ P do(i) X . Let Dn be the collection of marginal observations of (Xpa(n), Xn) drawn from any distribution in which Xn is not intervened upon.6 Since by assumption Xn ∼ fn(Xpa(n)) + En where the distribution of En ∼ N(0, σ2 n) is known, each element (xpa(n), xn) of Dn represents an evaluation of fn at the input point xpa(n) corrupted by Gaussian noise of known variance. Performing GP regression using 5 No specific assumptions will be made on the choice of kn, which can be freely chosen to incorporate prior knowledge about the functions (for instance, typical length scale of variation and magnitude). If Xn is parentless, fn is an unknown constant for which we assume a 1-dimensional Gaussian prior with zero mean and variance kn.\\n  |\\n   6 That is, for any distribution P do(i) X for which Xn 6∈ var(i).\\n  |\\n   4  Dn as the data gives the posterior distribution over fn. By properties of Gaussians, this is also a GP with distribution fn|Dn ∼ GP(µfn|Dn , kfn|Dn ), where µfn|Dn and kfn|Dn (x, y) can be explicitly written in terms of kn and the data Dn (see Appendix for details). The above procedure can be applied for each fn independently, giving a posterior distribution over the vector of functions f.\\n  |\\n   Which ˆ f should be chosen, given the posterior over f? Problem 1 demands that a single choice ˆ f be made when making the estimation c M of M. For a fixed ˆ f, the total risk L( ˆ f||f) is a random variable (the randomness coming from the uncertain belief over f). The expectation of this random variable can be calculated and expressed in terms of the posterior covariance and mean functions of each fn.\\n  |\\n   Lemma 1.\\n  |\\n   Ef|D h L( ˆ f||f) i = N X n=1 αn Z Xpa(n)  ˆ fn(x) − µfn|Dn (x) 2 + kn|Dn (x, x)dΠn(x) See Appendix for proof. This immediately implies the following result.\\n  |\\n   Lemma 2. Let µf|D be the tuple of functions (µfn|Dn )n=1,...,N . Then µf|D = arg min ˆ f Ef|D h L( ˆ f||f) i That is, choosing ˆ fn to be the posterior mean of fn for each n minimises the expected total risk.\\n  |\\n   The uncertain distribution over f directly yields an estimate of the total risk once the optimal ˆ f = µf|D is chosen which, once the prior has been fixed, is purely a function of the data D. Denote by R(D) = Ef|D  L(µf|D||f)  this expected total risk. Choosing the intervention i for which R(D ∪ {(i, x)}) is expected to be smallest after making the new observation x from P do(i) X forms the basis of the proposed active learning algorithm.\\n  |\\n   In this section a myopic active learning algorithm is derived based on the GP belief of the functions fn and the expected total risk R(D) described above. At each step in time, we select an intervention i at cost c(i) and observe a single draw from the distribution P do(i) X . The goal is to select the intervention i ∈ I which will reduce the expected total risk as much as possible, taking into account the cost c(i).\\n  |\\n   This problem is non-trivial for two main reasons.\\n  |\\n   1. The true distributions P do(i) X are unknown and therefore it is not possible to calculate the true expected reduction in expected total risk given a proposed intervention.\\n  |\\n   2. There is a potentially large set of interventions that must be searched over.\\n  |\\n   Consider the first issue above. How will the expected total risk change if the intervention i is chosen and a single new observation is drawn from P do(i) X ? If the new observation is x ∈ X, the new expected total risk will be R(D ∪ {(i, x)}). Define the value of the intervention i to be the expected reduction of R after performing the intervention i, divided by the cost of i: V (i|D) = R(D) − Ex∼P do(i) X R (D ∪ {(i, x)}) c(i) (?) The goal is to find the intervention with the largest value, but since P do(i) X is unknown it is not possible to calculate the right-hand term in the numerator of (?). It is possible, however, to estimate this by replacing P do(i) X in the expectation with the belief of the distribution based on the uncertain estimates of each fn. In this next two parts of this section, two different methods are proposed that estimate the expected total risk after performing each intervention. The first uses sampling, and requires a brute-force search over the set of interventions. This may be appropriate when the set of possible interventions is small enough that this is feasible. The second uses a form of dynamic programming, enabling a search over a larger set of interventions more efficiently. The derived algorithm, however, makes specific assumptions on the graph of M and the set of interventions.\\n  |\\n   5\\n  |\\n   risk after interventions (chain, interventions on all Xn≤m, some m ) 1: Input: Previously observed data D, GP kernels kn, discretisations b Xn of each Xn.\\n  |\\n   2: Pre-compute Un vectors and discrete approximations to conditional probability distributions: 3: for n = 1, . . . , N do 4: If n = 1: P 1 x1 ∝ P (x1) for x1 ∈ b X1, else: 5: P n xn−1,xn ∝ P (xn|xn−1) for xn−1 ∈ b Xn−1, xn ∈ b Xn 6: Ucurr n = Rn(Dn) 7: Un(xn−1) for xn−1 ∈ b Xn−1 8: end for 9: Calculate new expected risk for all interventions: 10: for n = 1, . . . , N do 11: V = 0 12: for m = N − 1, . . . , n + 1 do 13: V = P m (V + Um+1) 14: end for 15: Expected risk after intervention on variables Xm, m ≤ n: 16: ERn = V + Uold 1 + . . . + Uold n 17: end for 18: return Vectors ERn giving estimated expected risks after all interventions do(Xn = xn, Xn−1 = . . .) If the set of interventions under consideration exhibits structure that coincides with that of the causal graph appropriately, it is possible to estimate the value of many interventions simultaneously. A specific example is provided here of how this can be done in the case that the causal graph is a chain X1 → . . . → XN , and any intervention intervenes on one variable and everything upstream of it.8 A similar example for chains in which all interventions intervene on exactly one variable is provided in the Appendix.\\n  |\\n   The crux of this approach is the fact that the posterior covariance function of a Gaussian Process is only a function of the inputs of the conditioning data, not of the outputs. That is, writing 7 In contrast to b P do(i) X , which is the estimated distribution once a particular choice for ˆ f is made.\\n  |\\n   8 That is, any intervention is of the form do(Xn = xn, n ≤ m) for some m. This is equivalent to the case that all interventions act on a single variable, but only variables downstream of this are observable.\\n  |\\n   6  X0 X1 X2 X3 X4 (a) X0 X1 X2 X3 X4 (b) Figure 2: The causal graphs of the SCMs (a) M1 and (b) M2 used in the experiments (Section 6).\\n  |\\n   Rn(Dn) = Efn|Dn [Ln(µfn|Dn , fn)] = R Xpa(n) kn|Dn (x, x)dΠn(x) for the contribution to the expected total risk due to estimating function fn, and writing Dn = {(xs pa(n), xs n) : s = 1 . . . , |Dn|}, it follows that Rn is only actually a function of the xs pa(n) (or, for parentless variables, just the size of the dataset |Dn|). Consider the intervention i = do(Xm = xm, Xm−1 = . . .) that sets Xm = xm and all variables upstream of Xm to arbitrary values. When a new observation x ∼ P do(i) X is made, this only provides new information about the functions fn for n  m, since i intervenes on Xn for n ≤ m. Let Ucurr n = Rn(Dn) for n ≤ m be the current contributions to the expected total risk. Define, for n  m, the following shorthand for the contribution to the new expected total risk function made by fn if a new observation of fn at the input point xn−1 is made: Un(xn−1) = Rn(Dn ∪ {(xn−1, xn)}) for any value xn, n  m.\\n  |\\n   It follows that the estimated expected total risk after performing the intervention i decomposes thus: Ex∼e P do(i) X [R(D ∪ {(i, x)})] = m X n=1 Ucurr n + Ex∼e P do(i) X \" N X n=m+1 Un(xn−1) # . (†) By exploiting a factorisation of e P do(i) X similar to (∗) and discretely approximating the continuous domains Xn, it is possible to reduce evaluating the right hand side of (†) to a series of matrix multiplications and additions (see Appendix for details). This series of operations can be vectorised to allow calculation of (†) for many xm simultaneously. Moreover, many of the initial calculations can be cached and used to speed up calculation over different values of m. See Algorithm 2.\\n  |\\n   There are many ways in which the work presented here could be incrementally furthered: it may be possible to find efficient ways to search over the set of interventions subject to less restrictive assumptions than those made for Algorithm 2; different distributions over the noise variables could be considered, in which case an approximation may need to be made when regressing to find the posterior distribution over each fn; one could try to relax the additive noise assumption altogether.\\n  |\\n   Other natural extensions include estimating the value of an intervention based on reasoning multiple steps into the future, or considering the implications of a constrained budget.\\n  |\\n   Although the proposed algorithms seem to perform well on the synthetic toy examples considered, it remains to be seen whether this method, suitably extended, would similarly perform well on a convincing real-world problem. It is hard to find suitable real-world problems where convincing ground truth (for the functional relationships) exists, which is why we believe that it is sensible to assay such methods on synthetic data where performance can be accurately measured.\\n  |\\n   A perhaps more fundamental issue that was raised and not tackled is the fact that it is not clear how best one should even define what it means to learn an SCM, or the parameters thereof. We proposed supi∈I0 Li KL( ˆ f||f) and supi∈I0 Li MMDl ( ˆ f||f) for a suitable set of interventions I0 as potentially more principled objectives to minimise than the total risk functional considered here. Interestingly, the derived algorithms do reduce these quantities in the experiments considered, though this was in no way an explicit objective. Future directions of research include trying to understand whether these, or other objectives, give rise to tractable methods for parameter estimation in causal models and for selecting interventions in active settings, and under what assumptions mathematical guarantees can be made.\\n  |\\n   8\\n  |\\n   Similar reasoning to the above can be used to derive a dynamic programming scheme to calculate the estimated total risk after a proposed intervention i = do(Xm = x∗ m) (i. e. intervening on a single variable) for many x∗ m simultaneously. This is summarised by Algorithm 3.\\n  |\\n   Under this intervention, the joint distribution factorises as e P do(i) X = e PX1 m−1 Y n=2 e PXn|Xn−1 δXm=x∗ m N Y n=m+1 e PXn|Xn−1 .\\n  |\\n   11  When we intervene on a single variable Xm, we learn something new about all functions except fm. We can write the expected new total risk, the quantity we want to evaluate, as Ex∼e P do(i) X [R(D ∪ {(i, x)})] = U1 + Ex∼e P do(i) X \"m−1 X n=2 Un(xn−1) # + Ucurr m + Ex∼e P do(i) X \" N X n=m+1 Un(xn−1) # .\\n  |\\n   Each of the expectations above can be calculated recursively in a similar fashion to the strategy employed above.\\n  |\\n   If we define VN−1(xN−2) = Z XN−1 UN (xN−1) de P do(i) XN−1|XN−2 (xN−1|xN−2), Vn(xn−1) = Z Xn Vn+1(xn) + Un+1(xn) de P do(i) Xn|Xn−1 (xn|xn−1), n = N − 2, . . . m + 1, it follows that Vm+1(x∗ m) = Ex∼e P do(i) X \" N X n=m+1 Un(xn−1) # .\\n  |\\n   Similarly, defining Vm−2(xm−3) = Z Xm−2 Um−1(xm−2) de P do(i) Xm−2|Xm−3 (xm−2|xm−3), Vn(xn−1) = Z Xn Vn+1(xn) + Un+1(xn) de P do(i) Xn|Xn−1 (xn|xn−1), n = m − 3, . . . 2, V1 = Z X1 V2(x1) + U2(x1) de P do(i) X1 (x1), it follows that V1 = Ex∼e P do(i) X \"m−1 X n=2 Un(xn−1) # .\\n  |\\n   As before, we can approximate calculation of Vm+1(x∗ m) for many x∗ m simultaneously by discretising each Xn into a set of points x1 n, x2 n, . . . , xDn n . Define Pn to be the matrix with normalised rows such that Pn ij ∝ e p do(i) Xn|Xn−1 (xj n|xi n−1) for n  1, where e pdo(i) is the density of e Pdo(i) with respect to the Lebesgue measure on X. Define P1 to be the normalised vector with P1 i ∝ e pX1 (xi 1). Define un to be the vector with entries un i = Un(xi n−1). Then, if we recursively define vN−1 = PN−1 uN , vn = Pn (vn+1 + un+1) n = N − 2, . . . m + 1, it follows that vm+1 i ≈ Vm+1(xi m). We must also estimate V1. If we define vm−2 = Pm−2 um−2, vn = Pn (vn+1 + un+1) n = m − 3, . . . 2, v1 = P1| (v2 + u2) then it follows that v1 ≈ V1.\\n  |\\n   12  Algorithm 3 Dynamic programming to estimate expected risk after interventions (chain, interventions on single variable Xm) 1: Input: Previously observed data D, GP kernels kn, discretisations b Xn of each Xn.\\n  |\\n   2: Pre-compute Un vectors and discrete approximations to conditional probability distributions: 3: for n = 1, . . . , N do 4: P n xn−1,xn ∝ P (xn|xn−1) for xn−1 ∈ b Xn−1, xn ∈ b Xn 5: Ucurr n = Rn(Dn) 6: Un(xn−1) for xn−1 ∈ b Xn−1 7: end for 8: Calculate expected loss for all interventions on each variable in turn: 9: for m = 1, . . . , N do 10: V = 0 11: for n = N − 1, . . . , m + 1 do 12: V = P n (V + Un) 13: end for 14: V 0 = 0 15: for n = m − 1, . . . , 2 do 16: V 0 = P n (V 0 + Un) 17: end for 18: Expected risk after intervention on variable m: ERm = V + V 0 + U1 + Ucurr m 19: end for 20: return Vectors ERn giving estimated expected risks after interventions for all interventions on Xn.\\n  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check: ok!\n",
    "df_text.loc[df_text.paper_id == '100002']['text'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   We consider the problem of learning the functions computing children from parents in a Structural Causal Model once the underlying causal graph has been identified.\\n   \\n   This is in some sense the second step after causal discovery. Taking a probabilistic approach to estimating these functions, we derive a natural myopic active learning scheme that identifies the intervention which is optimally informative about all of the unknown functions jointly, given previously observed data. We test the derived algorithms on simple examples, to demonstrate that they produce a structured exploration policy that significantly improves on unstructured base-lines.\\n   \\n   Large parts of the literature on causality are concerned with learning the causal graph of a system of random variables [Spirtes et al., 2000, Tong and Koller, 2001, Eberhardt, 2010, Hyttinen et al., 2013, Mooij et al., 2016]. Also known as causal discovery or causal inference, this problem is motivated by realistic problems in science: a biologist may wish to discover genes responsible for regulating other genes in a cell; a public health researcher may wish to know whether certain habits in a population (e. g. smoking) influence certain health outcomes (e. g. probability of developing lung cancer).\\n   \\n   The starting point of this paper is to consider what should be done after the causal graph of a system of variables has already been identified (be it by causal discovery methods, or from prior knowledge).\\n   \\n   That is, it is known which variables are functions of which other variables, but the precise functional relationships are still unknown. Thus, although we understand the causal relationships in a coarse sense, we will not be able to accurately predict the result of an intervention to the system, possibly with implications for decision making.\\n   \\n   For instance, suppose that in a cell, Gene A up-regulates Gene B and Gene B down-regulates Gene C.\\n   \\n   We know that reducing expression of Gene A will lead to a decrease in expression of Gene B which, in turn, will lead to an increase in expression of Gene C. However, without more precise knowledge of the relationships between genes, we will be unable to quantitatively predict the effect of applying a drug that reduces expression of Gene A by 20%. Similarly, if our goal is to reduce overall levels of lung cancer in the population, then knowing only that smoking causes cancer is insufficient to know what the best public health policy should be: would it be better if we could persuade 50% of smokers to stop smoking completely, or persuade every smoker to reduce their consumption by 50%? For a passive agent supplied with data generated by the system, learning the functional relationships between parents and children reduces simply to separate regression problems—one for each unknown function—once the causal graph is known. Many knowledge acquisition problems, however, can be phrased as a sequential decision making process in which the data that is received at the next point in time is affected by a decision made based on the data that has already been observed. A biologist does not blindly perform a series of costly and time-consuming experiments, only looking at the generated data once the last is over; the data from each experiment would be analysed before the next is performed, thus informing which experiment would be best to perform next.\\n   \\n   arXiv:1706.10234v1 [stat.ML] 30 Jun 2017  X1 X2 X3 f2 f3 (a) The observational setting X1 X2 X3 f2 f3 (b) do(X1 = x1) X1 X2 X3 f3 (c) do(X2 = x2) Figure 1: Even in the simple setting of three variables whose graph is a chain, there is a non-trivial trade-off between the information one expects to gain by performing different interventions (see text).\\n   \\n   Below, we formalise this problem using the language of Structural Causal Models (SCMs), also known as Structural Equation Models [Pearl, 2009, Bollen, 2014]. An SCM, in essence, consists of functions connecting child variables with their causal parents, and is equipped with a notion of intervention in which a variable (or subset of variables) is externally forced to take a particular value (or values). We use these interventions as an idealised mathematical representation of performing an experiment. We take a Bayesian approach to estimating the functional relationships between parents and children, which naturally gives rise to an active learning algorithm to decide on the next ‘experiment’ to perform.\\n   \\n   While our approach works with causal graphs that are arbitrary DAGs, the non-triviality of this problem is apparent even in the simple case of three variables whose causal graph is a chain (Figure 1).\\n   \\n   Our goal in this situation is, in a sense to be made precise later, to learn the functions f2 and f3. At each point in time, we must decide whether to perform one of the possible interventions or to passively observe the system, with each different action having some cost. If we make a passive observation, we will learn something about both f2 and f3, though only in areas where the distributions over X1 and X2 put probability mass. In the small sample setting, we are very unlikely to learn anything about the functions in areas of low probability of their inputs. If we intervene on X1 and choose what value to set it to, we can decide precisely where we want to learn about f2 and we will also learn about f3 in some region, although we would be uncertain about where. If we intervene on X2, we can learn a precise aspect of f3, but will learn nothing about f2. How should we decide which action to pick, given what we already know about f2 and f3? The problem considered in this paper and our approach to solving it have a close connection to ideas in Bayesian optimization [Jones et al., 1998, Osborne et al., 2009, Shahriari et al., 2016]. In Bayesian optimization, the goal is to find the extremum of a function that is expensive to evaluate, possibly exploiting known structure to speed up the search. Due to this expense, the information obtained from each evaluation should be used efficiently. In contrast, however, in our setting we are not simply interested in finding the extremum of an unknown function, but rather to learn the entire function (or set of functions) in some sense. Tong and Koller [2000] considered a similar setting, but only treated discrete variables.\\n   \\n   Below, we begin with a formal definition of Structural Causal Models (Section 2). Section 3 states the precise problem we are tackling. Section 4 provides a Bayesian formulation for inference in this setting, which is then complemented by an active learning scheme to guide exploration (Section 5).\\n   \\n   Section 6 provides empirical evaluations on synthetic toy examples.\\n   \\n   Suppose there exists an SCM M = (S, I, PE) with S = {Xn = fn(Xpa(n)) + En : n = 1, . . . , N} and PE ∼ N(0, Λ) where Λ is diagonal. We assume that the graphical structure is known, but the functions fn themselves are not. For simplicity, we assume that Λ is known.3 We are given data D drawn from the observational and a variety of interventional distributions of M; each element of D is a tuple (i, x), where x is an independent draw from P do(i) X . We are interested in two separate tasks.\\n   \\n   Problem 1: Estimating M. Using D, learn functions ˆ fn such that the estimated model c M = (Ŝ, I, PE) with Ŝ = {Xn = ˆ fn(Xpa(n)) + En : n = 1, . . . , N} is ‘close’ to the true model M in some sense. Part of this problem is to define a sensible notion of closeness between SCMs.\\n   \\n   Problem 2: Active learning. We can select an intervention i ∈ I at some cost c(i) and observe a single draw from P do(i) X . Which i should be selected to ensure the next estimation c M after incorporating the new datum is as close to M as possible? The rest of this section is devoted to defining a risk functional to provide a notion of closeness between c M and M. There may be no single best way to define this notion of closeness, as desirable properties may be dependent on particular use case.4 Consider, for instance, the following scenarios in which the ultimate goal is to: • Approximate each function fn so that a practitioner can visually interpret the relationship between parents and children (e.g., identifying genes as ‘excitatory’ or ‘inhibitory’, or identifying a threshold dosage at which a drug under medical trial is considered toxic.).\\n   \\n   • Predict the result of an intervention that cannot be practically carried out, or is potentially dangerous to do so (e.g., raising interest rates, or giving a patient a drug).\\n   \\n   1 That is, the directed graph with nodes {1, . . . , N} with edges n → m if and only if n ∈ pa(m) is a DAG.\\n   \\n   2 That is, for any e ∈ E, there is a unique x ∈ X such that xn = fn(xpa(n), en) for each n. This can be seen to be true by explicitly writing each xn as a function of the en by substituting the equations into one another.\\n   \\n   3 This assumption could be relaxed to a more Bayesian approach involving a prior over the covariance matrix, which would reduce to running the algorithm derived in the following, but averaging its results over the posterior.\\n   \\n   4 The same is true in the case of causal graph learning. See e. g. de Jongh and Druzdzel [2009].\\n   \\n   3  • Better control a system in a variety of environments or conditions (e.g., learning the dynamics of a complex vehicle to be employed in variable conditions) We will suppose that for each function fn with domain Xpa(n) = Q m∈pa(n) Xm we are supplied with a probability measure Πn over Xpa(n) specifying the importance of learning the pointwise value of fn at each input xpa(n) ∈ Xpa(n). That is, Πn puts large amounts of mass in areas that we should learn fn precisely, small amounts of mass in areas that we should learn fn only approximately, and zero mass in areas for which we do not care about learning fn at all. For the (estimated) function ˆ fn, we use the risk functional Ln below. The weighted sum of these according to the importance of each function gives rise to the total risk L, where f = (fn)n=1,...,N and ˆ f = ( ˆ fn)n=1,...,N are vectors of the functions fn and ˆ fn, respectively.\\n   \\n   Ln( ˆ fn  fn) = Z Xpa(n)  fn(x) − ˆ fn(x) 2 dΠn(x), L( ˆ f  f) = N X n=1 αnLn( ˆ fn  fn), αn ≥ 0.\\n   \\n   We will assume for simplicity that αn = 1 for each n. Ln is also known as the Mean Integrated Squared Error [Tsybakov, 2009] and in the case that Πn = P do(∅) Xpa(n) , this coincides with a typical objective that would be minimised in a classical non-parametric statistical learning setting [Györfi et al., 2006]. It is worth considering other possible risk functionals that could be used, since the presence of the measures Πn is arguably somewhat arbitrary in the L that we consider. When learning the parameters of a statistical model, a commonly used objective with many separate justifications is to minimise the KL divergence KL[PX  P̂X] between the true data distribution PX and that implied by the learned model, P̂X. SCMs do not imply a single distribution over the variables X, but rather a family of distributions, one for each intervention: {P do(i) X : i ∈ I}. One may therefore wish to consider a separate loss for each interventional distribution and, for example, uniformly bound these losses over a subset of interventions I0 ⊆ I of interest.\\n   \\n   Li KL( ˆ f  f) = KL h P do(i) X   P̂ do(i) X i , LI0 KL( ˆ f  f) = sup i∈I0 Li KL( ˆ f  f).\\n   \\n   Alternatively, one could replace the KL divergence with a different divergence measure or metric on distributions. For instance, one could use the Maximum Mean Discrepancy (MMD) corresponding to a characteristic kernel l [Sriperumbudur et al., 2008] Li MMDl ( ˆ f  f) = MMDl h P do(i) X   P̂ do(i) X i , LI0 MMDl ( ˆ f  f) = sup i∈I0 Li MMDl ( ˆ f  f).\\n   \\n   Though we do not analyse or derive active learning schemes for these risk functionals, they will be used in Section 6 to evaluate our algorithm. We leave their consideration for future work.\\n   \\n   By taking a Bayesian approach to learning the vector of unknown functions f, Problem 1 can be reduced to a series of independent regression problems between input and output domains Xpa(n) and Xn for each n. A common choice of prior when learning functions is a Gaussian Process (GP) [Rasmussen and Williams, 2005]. For each function fn, we will assume a zero mean GP prior with kernel kn over the domain Xpa(n) 5 fn ∼ GP(0, kn).\\n   \\n   Recall that we are given a dataset D consisting of elements (i, x) where x ∼ P do(i) X . Let Dn be the collection of marginal observations of (Xpa(n), Xn) drawn from any distribution in which Xn is not intervened upon.6 Since by assumption Xn ∼ fn(Xpa(n)) + En where the distribution of En ∼ N(0, σ2 n) is known, each element (xpa(n), xn) of Dn represents an evaluation of fn at the input point xpa(n) corrupted by Gaussian noise of known variance. Performing GP regression using 5 No specific assumptions will be made on the choice of kn, which can be freely chosen to incorporate prior knowledge about the functions (for instance, typical length scale of variation and magnitude). If Xn is parentless, fn is an unknown constant for which we assume a 1-dimensional Gaussian prior with zero mean and variance kn.\\n   \\n   6 That is, for any distribution P do(i) X for which Xn 6∈ var(i).\\n   \\n   4  Dn as the data gives the posterior distribution over fn. By properties of Gaussians, this is also a GP with distribution fn Dn ∼ GP(µfn Dn , kfn Dn ), where µfn Dn and kfn Dn (x, y) can be explicitly written in terms of kn and the data Dn (see Appendix for details). The above procedure can be applied for each fn independently, giving a posterior distribution over the vector of functions f.\\n   \\n   Which ˆ f should be chosen, given the posterior over f? Problem 1 demands that a single choice ˆ f be made when making the estimation c M of M. For a fixed ˆ f, the total risk L( ˆ f  f) is a random variable (the randomness coming from the uncertain belief over f). The expectation of this random variable can be calculated and expressed in terms of the posterior covariance and mean functions of each fn.\\n   \\n   Lemma 1.\\n   \\n   Ef D h L( ˆ f  f) i = N X n=1 αn Z Xpa(n)  ˆ fn(x) − µfn Dn (x) 2 + kn Dn (x, x)dΠn(x) See Appendix for proof. This immediately implies the following result.\\n   \\n   Lemma 2. Let µf D be the tuple of functions (µfn Dn )n=1,...,N . Then µf D = arg min ˆ f Ef D h L( ˆ f  f) i That is, choosing ˆ fn to be the posterior mean of fn for each n minimises the expected total risk.\\n   \\n   The uncertain distribution over f directly yields an estimate of the total risk once the optimal ˆ f = µf D is chosen which, once the prior has been fixed, is purely a function of the data D. Denote by R(D) = Ef D  L(µf D  f)  this expected total risk. Choosing the intervention i for which R(D ∪ {(i, x)}) is expected to be smallest after making the new observation x from P do(i) X forms the basis of the proposed active learning algorithm.\\n   \\n   In this section a myopic active learning algorithm is derived based on the GP belief of the functions fn and the expected total risk R(D) described above. At each step in time, we select an intervention i at cost c(i) and observe a single draw from the distribution P do(i) X . The goal is to select the intervention i ∈ I which will reduce the expected total risk as much as possible, taking into account the cost c(i).\\n   \\n   This problem is non-trivial for two main reasons.\\n   \\n   1. The true distributions P do(i) X are unknown and therefore it is not possible to calculate the true expected reduction in expected total risk given a proposed intervention.\\n   \\n   2. There is a potentially large set of interventions that must be searched over.\\n   \\n   Consider the first issue above. How will the expected total risk change if the intervention i is chosen and a single new observation is drawn from P do(i) X ? If the new observation is x ∈ X, the new expected total risk will be R(D ∪ {(i, x)}). Define the value of the intervention i to be the expected reduction of R after performing the intervention i, divided by the cost of i: V (i D) = R(D) − Ex∼P do(i) X R (D ∪ {(i, x)}) c(i) (?) The goal is to find the intervention with the largest value, but since P do(i) X is unknown it is not possible to calculate the right-hand term in the numerator of (?). It is possible, however, to estimate this by replacing P do(i) X in the expectation with the belief of the distribution based on the uncertain estimates of each fn. In this next two parts of this section, two different methods are proposed that estimate the expected total risk after performing each intervention. The first uses sampling, and requires a brute-force search over the set of interventions. This may be appropriate when the set of possible interventions is small enough that this is feasible. The second uses a form of dynamic programming, enabling a search over a larger set of interventions more efficiently. The derived algorithm, however, makes specific assumptions on the graph of M and the set of interventions.\\n   \\n   5\\n   \\n   risk after interventions (chain, interventions on all Xn≤m, some m ) 1: Input: Previously observed data D, GP kernels kn, discretisations b Xn of each Xn.\\n   \\n   2: Pre-compute Un vectors and discrete approximations to conditional probability distributions: 3: for n = 1, . . . , N do 4: If n = 1: P 1 x1 ∝ P (x1) for x1 ∈ b X1, else: 5: P n xn−1,xn ∝ P (xn xn−1) for xn−1 ∈ b Xn−1, xn ∈ b Xn 6: Ucurr n = Rn(Dn) 7: Un(xn−1) for xn−1 ∈ b Xn−1 8: end for 9: Calculate new expected risk for all interventions: 10: for n = 1, . . . , N do 11: V = 0 12: for m = N − 1, . . . , n + 1 do 13: V = P m (V + Um+1) 14: end for 15: Expected risk after intervention on variables Xm, m ≤ n: 16: ERn = V + Uold 1 + . . . + Uold n 17: end for 18: return Vectors ERn giving estimated expected risks after all interventions do(Xn = xn, Xn−1 = . . .) If the set of interventions under consideration exhibits structure that coincides with that of the causal graph appropriately, it is possible to estimate the value of many interventions simultaneously. A specific example is provided here of how this can be done in the case that the causal graph is a chain X1 → . . . → XN , and any intervention intervenes on one variable and everything upstream of it.8 A similar example for chains in which all interventions intervene on exactly one variable is provided in the Appendix.\\n   \\n   The crux of this approach is the fact that the posterior covariance function of a Gaussian Process is only a function of the inputs of the conditioning data, not of the outputs. That is, writing 7 In contrast to b P do(i) X , which is the estimated distribution once a particular choice for ˆ f is made.\\n   \\n   8 That is, any intervention is of the form do(Xn = xn, n ≤ m) for some m. This is equivalent to the case that all interventions act on a single variable, but only variables downstream of this are observable.\\n   \\n   6  X0 X1 X2 X3 X4 (a) X0 X1 X2 X3 X4 (b) Figure 2: The causal graphs of the SCMs (a) M1 and (b) M2 used in the experiments (Section 6).\\n   \\n   Rn(Dn) = Efn Dn [Ln(µfn Dn , fn)] = R Xpa(n) kn Dn (x, x)dΠn(x) for the contribution to the expected total risk due to estimating function fn, and writing Dn = {(xs pa(n), xs n) : s = 1 . . . ,  Dn }, it follows that Rn is only actually a function of the xs pa(n) (or, for parentless variables, just the size of the dataset  Dn ). Consider the intervention i = do(Xm = xm, Xm−1 = . . .) that sets Xm = xm and all variables upstream of Xm to arbitrary values. When a new observation x ∼ P do(i) X is made, this only provides new information about the functions fn for n  m, since i intervenes on Xn for n ≤ m. Let Ucurr n = Rn(Dn) for n ≤ m be the current contributions to the expected total risk. Define, for n  m, the following shorthand for the contribution to the new expected total risk function made by fn if a new observation of fn at the input point xn−1 is made: Un(xn−1) = Rn(Dn ∪ {(xn−1, xn)}) for any value xn, n  m.\\n   \\n   It follows that the estimated expected total risk after performing the intervention i decomposes thus: Ex∼e P do(i) X [R(D ∪ {(i, x)})] = m X n=1 Ucurr n + Ex∼e P do(i) X \" N X n=m+1 Un(xn−1) # . (†) By exploiting a factorisation of e P do(i) X similar to (∗) and discretely approximating the continuous domains Xn, it is possible to reduce evaluating the right hand side of (†) to a series of matrix multiplications and additions (see Appendix for details). This series of operations can be vectorised to allow calculation of (†) for many xm simultaneously. Moreover, many of the initial calculations can be cached and used to speed up calculation over different values of m. See Algorithm 2.\\n   \\n   There are many ways in which the work presented here could be incrementally furthered: it may be possible to find efficient ways to search over the set of interventions subject to less restrictive assumptions than those made for Algorithm 2; different distributions over the noise variables could be considered, in which case an approximation may need to be made when regressing to find the posterior distribution over each fn; one could try to relax the additive noise assumption altogether.\\n   \\n   Other natural extensions include estimating the value of an intervention based on reasoning multiple steps into the future, or considering the implications of a constrained budget.\\n   \\n   Although the proposed algorithms seem to perform well on the synthetic toy examples considered, it remains to be seen whether this method, suitably extended, would similarly perform well on a convincing real-world problem. It is hard to find suitable real-world problems where convincing ground truth (for the functional relationships) exists, which is why we believe that it is sensible to assay such methods on synthetic data where performance can be accurately measured.\\n   \\n   A perhaps more fundamental issue that was raised and not tackled is the fact that it is not clear how best one should even define what it means to learn an SCM, or the parameters thereof. We proposed supi∈I0 Li KL( ˆ f  f) and supi∈I0 Li MMDl ( ˆ f  f) for a suitable set of interventions I0 as potentially more principled objectives to minimise than the total risk functional considered here. Interestingly, the derived algorithms do reduce these quantities in the experiments considered, though this was in no way an explicit objective. Future directions of research include trying to understand whether these, or other objectives, give rise to tractable methods for parameter estimation in causal models and for selecting interventions in active settings, and under what assumptions mathematical guarantees can be made.\\n   \\n   8\\n   \\n   Similar reasoning to the above can be used to derive a dynamic programming scheme to calculate the estimated total risk after a proposed intervention i = do(Xm = x∗ m) (i. e. intervening on a single variable) for many x∗ m simultaneously. This is summarised by Algorithm 3.\\n   \\n   Under this intervention, the joint distribution factorises as e P do(i) X = e PX1 m−1 Y n=2 e PXn Xn−1 δXm=x∗ m N Y n=m+1 e PXn Xn−1 .\\n   \\n   11  When we intervene on a single variable Xm, we learn something new about all functions except fm. We can write the expected new total risk, the quantity we want to evaluate, as Ex∼e P do(i) X [R(D ∪ {(i, x)})] = U1 + Ex∼e P do(i) X \"m−1 X n=2 Un(xn−1) # + Ucurr m + Ex∼e P do(i) X \" N X n=m+1 Un(xn−1) # .\\n   \\n   Each of the expectations above can be calculated recursively in a similar fashion to the strategy employed above.\\n   \\n   If we define VN−1(xN−2) = Z XN−1 UN (xN−1) de P do(i) XN−1 XN−2 (xN−1 xN−2), Vn(xn−1) = Z Xn Vn+1(xn) + Un+1(xn) de P do(i) Xn Xn−1 (xn xn−1), n = N − 2, . . . m + 1, it follows that Vm+1(x∗ m) = Ex∼e P do(i) X \" N X n=m+1 Un(xn−1) # .\\n   \\n   Similarly, defining Vm−2(xm−3) = Z Xm−2 Um−1(xm−2) de P do(i) Xm−2 Xm−3 (xm−2 xm−3), Vn(xn−1) = Z Xn Vn+1(xn) + Un+1(xn) de P do(i) Xn Xn−1 (xn xn−1), n = m − 3, . . . 2, V1 = Z X1 V2(x1) + U2(x1) de P do(i) X1 (x1), it follows that V1 = Ex∼e P do(i) X \"m−1 X n=2 Un(xn−1) # .\\n   \\n   As before, we can approximate calculation of Vm+1(x∗ m) for many x∗ m simultaneously by discretising each Xn into a set of points x1 n, x2 n, . . . , xDn n . Define Pn to be the matrix with normalised rows such that Pn ij ∝ e p do(i) Xn Xn−1 (xj n xi n−1) for n  1, where e pdo(i) is the density of e Pdo(i) with respect to the Lebesgue measure on X. Define P1 to be the normalised vector with P1 i ∝ e pX1 (xi 1). Define un to be the vector with entries un i = Un(xi n−1). Then, if we recursively define vN−1 = PN−1 uN , vn = Pn (vn+1 + un+1) n = N − 2, . . . m + 1, it follows that vm+1 i ≈ Vm+1(xi m). We must also estimate V1. If we define vm−2 = Pm−2 um−2, vn = Pn (vn+1 + un+1) n = m − 3, . . . 2, v1 = P1  (v2 + u2) then it follows that v1 ≈ V1.\\n   \\n   12  Algorithm 3 Dynamic programming to estimate expected risk after interventions (chain, interventions on single variable Xm) 1: Input: Previously observed data D, GP kernels kn, discretisations b Xn of each Xn.\\n   \\n   2: Pre-compute Un vectors and discrete approximations to conditional probability distributions: 3: for n = 1, . . . , N do 4: P n xn−1,xn ∝ P (xn xn−1) for xn−1 ∈ b Xn−1, xn ∈ b Xn 5: Ucurr n = Rn(Dn) 6: Un(xn−1) for xn−1 ∈ b Xn−1 7: end for 8: Calculate expected loss for all interventions on each variable in turn: 9: for m = 1, . . . , N do 10: V = 0 11: for n = N − 1, . . . , m + 1 do 12: V = P n (V + Un) 13: end for 14: V 0 = 0 15: for n = m − 1, . . . , 2 do 16: V 0 = P n (V 0 + Un) 17: end for 18: Expected risk after intervention on variable m: ERm = V + V 0 + U1 + Ucurr m 19: end for 20: return Vectors ERn giving estimated expected risks after interventions for all interventions on Xn.\\n  '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['text'] = df_text.text.apply(lambda x: x.replace('|', ' '))\n",
    "df_text.loc[df_text.paper_id == '100002']['text'].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../resources/interesting_arxiv_papers.pkl\"\n",
    "df_text.to_pickle(dataset_path, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
