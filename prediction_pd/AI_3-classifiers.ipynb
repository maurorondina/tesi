{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Text-Clean Training Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>label_subsection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2549.1.1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>work machin learn extract focus distinct subpr...</td>\n",
       "      <td>N_PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2549.2.1</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>extract problem convert text newswir articl we...</td>\n",
       "      <td>N_PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2549.2.2</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>increas import brought attent kind automat doc...</td>\n",
       "      <td>N_PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2549.2.3</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>work focus learn approach requir linguist expl...</td>\n",
       "      <td>N_PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2549.2.4</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>time work integr led need special wrapper proc...</td>\n",
       "      <td>N_PD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_subsection paragraph_name  \\\n",
       "0      2549.1.1       Abstract   \n",
       "1      2549.2.1   Introduction   \n",
       "2      2549.2.2   Introduction   \n",
       "3      2549.2.3   Introduction   \n",
       "4      2549.2.4   Introduction   \n",
       "\n",
       "                                     text_subsection label_subsection  \n",
       "0  work machin learn extract focus distinct subpr...             N_PD  \n",
       "1  extract problem convert text newswir articl we...             N_PD  \n",
       "2  increas import brought attent kind automat doc...             N_PD  \n",
       "3  work focus learn approach requir linguist expl...             N_PD  \n",
       "4  time work integr led need special wrapper proc...             N_PD  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_path = \"./resources/lemmatized_training_set.pkl\"\n",
    "dataset_path = \"./resources/stemmed_training_set.pkl\"\n",
    "df = pd.read_pickle(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_subsection</th>\n",
       "      <th>paragraph_name</th>\n",
       "      <th>text_subsection</th>\n",
       "      <th>label_subsection</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2549.1.1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>work machin learn extract focus distinct subpr...</td>\n",
       "      <td>N_PD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2549.2.1</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>extract problem convert text newswir articl we...</td>\n",
       "      <td>N_PD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2549.2.2</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>increas import brought attent kind automat doc...</td>\n",
       "      <td>N_PD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2549.2.3</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>work focus learn approach requir linguist expl...</td>\n",
       "      <td>N_PD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2549.2.4</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>time work integr led need special wrapper proc...</td>\n",
       "      <td>N_PD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132703</th>\n",
       "      <td>101131.17.2</td>\n",
       "      <td>C REPRODUCIBILITY</td>\n",
       "      <td>review confer paper iclr main task network rew...</td>\n",
       "      <td>N_PD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132711</th>\n",
       "      <td>101144.2.6</td>\n",
       "      <td>1. Introduction</td>\n",
       "      <td>approach preserv spatial spectral tempor struc...</td>\n",
       "      <td>PD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132712</th>\n",
       "      <td>101144.2.7</td>\n",
       "      <td>1. Introduction</td>\n",
       "      <td>increas effort automat detect phase start time...</td>\n",
       "      <td>PD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132713</th>\n",
       "      <td>101144.2.8</td>\n",
       "      <td>1. Introduction</td>\n",
       "      <td>sampl data descript data numer eeg read partic...</td>\n",
       "      <td>PD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132742</th>\n",
       "      <td>101144.7.1</td>\n",
       "      <td>6. Acknowledgements</td>\n",
       "      <td>thank auton lab professor artur dubrawski prov...</td>\n",
       "      <td>PD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96380 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_subsection       paragraph_name  \\\n",
       "0           2549.1.1             Abstract   \n",
       "1           2549.2.1         Introduction   \n",
       "2           2549.2.2         Introduction   \n",
       "3           2549.2.3         Introduction   \n",
       "4           2549.2.4         Introduction   \n",
       "...              ...                  ...   \n",
       "132703   101131.17.2    C REPRODUCIBILITY   \n",
       "132711    101144.2.6      1. Introduction   \n",
       "132712    101144.2.7      1. Introduction   \n",
       "132713    101144.2.8      1. Introduction   \n",
       "132742    101144.7.1  6. Acknowledgements   \n",
       "\n",
       "                                          text_subsection label_subsection  \\\n",
       "0       work machin learn extract focus distinct subpr...             N_PD   \n",
       "1       extract problem convert text newswir articl we...             N_PD   \n",
       "2       increas import brought attent kind automat doc...             N_PD   \n",
       "3       work focus learn approach requir linguist expl...             N_PD   \n",
       "4       time work integr led need special wrapper proc...             N_PD   \n",
       "...                                                   ...              ...   \n",
       "132703  review confer paper iclr main task network rew...             N_PD   \n",
       "132711  approach preserv spatial spectral tempor struc...               PD   \n",
       "132712  increas effort automat detect phase start time...               PD   \n",
       "132713  sampl data descript data numer eeg read partic...               PD   \n",
       "132742  thank auton lab professor artur dubrawski prov...               PD   \n",
       "\n",
       "        label_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "132703         0  \n",
       "132711         1  \n",
       "132712         1  \n",
       "132713         1  \n",
       "132742         1  \n",
       "\n",
       "[96380 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y.replace({'PD': 0, 'N_PD': 1}, inplace=True)\n",
    "#df.dtypes\n",
    "df['label_id'] = df['label_subsection'].factorize()[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N_PD': 0, 'PD': 1} {0: 'N_PD', 1: 'PD'}\n"
     ]
    }
   ],
   "source": [
    "label_df = df[['label_subsection', 'label_id']].drop_duplicates().sort_values('label_id')\n",
    "label_dict = dict(label_df.values)\n",
    "label_id_dict = dict(label_df[['label_id', 'label_subsection']].values)\n",
    "print(label_dict, label_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Extracion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 10.2 s, total: 40.9 s\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = df['text_subsection']\n",
    "\n",
    "vectorizer_modes = ['tdidf_trigr', 'tdidf_bigr', 'tdidf', 'count']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vectorizers = {\n",
    "    'tdidf_trigr': {'name': 'TF-IDF_trigrams  ',\n",
    "                   'vect': TfidfVectorizer(max_features=40000, min_df=3, norm='l2', ngram_range=(1, 2))\n",
    "    },\n",
    "    'tdidf_bigr': {'name': 'TF-IDF_bigrams  ',\n",
    "                   'vect': TfidfVectorizer(max_features=40000, min_df=3, norm='l2', ngram_range=(1, 2))\n",
    "    },\n",
    "    'tdidf': {'name': 'TF-IDF          ',\n",
    "              'vect': TfidfVectorizer(max_features=40000, norm='l2')\n",
    "    },\n",
    "    'count': {'name': 'BoW             ',\n",
    "              'vect': CountVectorizer(max_features=40000)\n",
    "    }\n",
    "}\n",
    "\n",
    "for vect_mode in vectorizer_modes:\n",
    "    vectorizers[vect_mode]['vectorizer'] = vectorizers[vect_mode]['vect'] #max_features=1500, min_df=5, max_df=0.7\n",
    "    vectorizers[vect_mode]['features'] = vectorizers[vect_mode]['vectorizer'].fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdidf_trigr: 40000\n",
      "(96380, 40000)\n",
      "tdidf_bigr: 40000\n",
      "(96380, 40000)\n",
      "tdidf: 40000\n",
      "(96380, 40000)\n",
      "count: 40000\n",
      "(96380, 40000)\n"
     ]
    }
   ],
   "source": [
    "for vect_mode in vectorizer_modes:\n",
    "    print(vect_mode +':', len(vectorizers[vect_mode]['vectorizer'].vocabulary_))\n",
    "    #print(len(vectorizers[vect_mode]['vectorizer'].get_feature_names()))\n",
    "    print(vectorizers[vect_mode]['features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TF-IDF_trigrams  :\t'N_PD'\n",
      "  . Most Correlated UNIGRAMS:\n",
      ". lot, presum, regardless, singlelabel, itembas, sabato, unselect, relianc, cambridg, mirror, zl, curvilinear, exacerb, creat, rq, wave, encourag, feder, demo, timeofday\n",
      "  . Most Correlated BIGRAMS:\n",
      ". domain includ, error use, embed learn, polici chang, equat give, approach optim, discret structur, tube predict, network present, demonstr potenti, propos comput, infer result, time featur, train phase, mean map, rout problem, issu address, plan use, dyngraphvecrnn dyngraphvecaernn, function need\n",
      "  . Most Correlated TRIGRAMS:\n",
      ". \n",
      "\n",
      "# TF-IDF_trigrams  :\t'PD'\n",
      "  . Most Correlated UNIGRAMS:\n",
      ". defin, rd, cbr, mmt, lovasz, definit, xi, nurs, logo, arcconsist, assum, given, notat, multiteam, formal, goal, denot, descript, problem, statement\n",
      "  . Most Correlated BIGRAMS:\n",
      ". notat problem, arg pr, statement introduc, preced constraint, goal learn, denot set, statement assum, formal defin, statement goal, mission multiteam, lovasz extens, statement paper, given set, statement problem, multiteam format, formal problem, statement consid, statement given, problem descript, problem statement\n",
      "  . Most Correlated TRIGRAMS:\n",
      ". \n",
      "\n",
      "CPU times: user 1min 7s, sys: 1min 40s, total: 2min 47s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import chi2\n",
    "N = 20\n",
    "\n",
    "### all vectorizer_modes: Kernel dies\n",
    "\n",
    "#for m in vectorizer_modes:\n",
    "#    for _label, _id in sorted(label_dict.items()):\n",
    "#        features_chi2 = chi2(vectorizers[m]['features'], df.label_id==_id)\n",
    "#        print(features_chi2)\n",
    "#        print(m, \"-\"*40)\n",
    "#        indices = np.argsort(features_chi2[0])\n",
    "#        feature_names = np.array(vectorizers[m]['vectorizer'].get_feature_names())[indices]\n",
    "#        unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#        bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#        trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "#        print(\"# {}:\\t'{}'\".format(vectorizers[m]['name'], _label))\n",
    "#        print(\"  . Most Correlated UNIGRAMS:\\n. {}\".format(', '.join(unigrams[-N:] if _id == 1 else unigrams[:N])))\n",
    "#        print(\"  . Most Correlated BIGRAMS:\\n. {}\".format(', '.join(bigrams[-N:] if _id == 1 else bigrams[:N])))\n",
    "#        print(\"  . Most Correlated TRIGRAMS:\\n. {}\\n\".format(', '.join(trigrams[-N:] if _id == 1 else trigrams[:N])))\n",
    "\n",
    "\n",
    "### only 'tdidf_trigr'\n",
    "\n",
    "for _label, _id in sorted(label_subsection_dict.items()):\n",
    "    features_chi2 = chi2(vectorizers['tdidf_trigr']['features'], df.label_id == _id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(vectorizers['tdidf_trigr']['vectorizer'].get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "    print(\"# {}:\\t'{}'\".format(vectorizers['tdidf_trigr']['name'], _label))\n",
    "    print(\"  . Most Correlated UNIGRAMS:\\n. {}\".format(', '.join(unigrams[-N:] if _id == 1 else unigrams[:N])))\n",
    "    print(\"  . Most Correlated BIGRAMS:\\n. {}\".format(', '.join(bigrams[-N:] if _id == 1 else bigrams[:N])))\n",
    "    print(\"  . Most Correlated TRIGRAMS:\\n. {}\\n\".format(', '.join(trigrams[-N:] if _id == 1 else trigrams[:N])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model_modes = ['lr', 'svc', 'rf', 'mnb']\n",
    "models = {\n",
    "    'lr': {'name': 'Logistic Regression ',\n",
    "           'estimator': LogisticRegression(random_state=0)\n",
    "    },\n",
    "    'rf': {'name': 'Random Forest       ',\n",
    "           'estimator': RandomForestClassifier(n_estimators=250, criterion='gini', max_depth=None, random_state=0)\n",
    "    },\n",
    "    'svc': {'name': 'Linear SVC         ',\n",
    "           'estimator': LinearSVC()\n",
    "    },\n",
    "    'mnb': {'name': 'Multinomial NB     ',\n",
    "           'estimator': MultinomialNB()\n",
    "    }\n",
    "}\n",
    "\n",
    "vectorizer_modes = ['tdidf_bigr', 'tdidf', 'count']\n",
    "#model_modes = ['lr']\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(model_modes) * len(vectorizer_modes)))\n",
    "entries = []\n",
    "for vect_mode in vectorizer_modes:\n",
    "    for model_mode in model_modes:\n",
    "        model = models[model_mode]['estimator']\n",
    "        features = vectorizers[vect_mode]['features']\n",
    "        labels = df['label_id']\n",
    "        f1_scores = cross_val_score(model, features, labels, scoring='f1', cv=CV) # all the estimators are classifiers, so StratifiedKFold is used\n",
    "        accuracy_scores = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "        fold_idx=0\n",
    "        for f1_score, accuracy_score in zip(f1_scores,accuracy_scores):\n",
    "            entries.append((vectorizers[vect_mode]['name'].strip(), models[model_mode]['name'].strip(), fold_idx, f1_score, accuracy_score))\n",
    "            fold_idx+=1\n",
    "        cv_df = pd.DataFrame(entries, columns=['vectorize_name', 'model_name', 'fold_idx', 'f1_score', 'accuracy_score'])\n",
    "        print(\"%s - %s : done.\" %(vect_mode, model_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_f1_acc = cv_df.groupby(['model_name', 'vectorize_name']).mean().drop('fold_idx', axis=1)\n",
    "cv_df_f1_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./resources/cross_validation_df.pkl\"\n",
    "cv_df_f1_acc.to_pickle(dataset_path, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
